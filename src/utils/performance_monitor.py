"""
Performance Monitoring System - Phase 5 Refactor
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import time
import psutil
import json
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
import os


@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    timestamp: datetime
    operation_name: str
    execution_time_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success: bool
    error_message: Optional[str] = None
    additional_data: Optional[Dict[str, Any]] = None


@dataclass
class BenchmarkTarget:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç›®æ¨™å€¤"""
    operation_name: str
    target_time_ms: float
    description: str
    critical: bool = True  # é‡è¦åº¦ï¼ˆç›®æ¨™æœªé”æˆæ™‚ã®ã‚¢ãƒ©ãƒ¼ãƒˆï¼‰


class PerformanceMonitor:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
    
    Features:
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§èƒ½æ¸¬å®š
    - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ
    - ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ°¸ç¶šåŒ–
    - ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½
    - çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """
    
    def __init__(self, storage_path: str = "benchmarks/"):
        self.logger = logging.getLogger(__name__)
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        
        # Phase 5 ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç›®æ¨™å€¤
        self.benchmark_targets = {
            "batch_embedding_250": BenchmarkTarget(
                operation_name="batch_embedding_250",
                target_time_ms=30000,  # 30ç§’
                description="ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿250ä»¶å‡¦ç†",
                critical=True
            ),
            "memory_migration_1000": BenchmarkTarget(
                operation_name="memory_migration_1000",
                target_time_ms=60000,  # 60ç§’
                description="ãƒ¡ãƒ¢ãƒªç§»è¡Œ1000ä»¶å‡¦ç†",
                critical=True
            ),
            "similarity_search": BenchmarkTarget(
                operation_name="similarity_search",
                target_time_ms=500,    # 500ms
                description="é¡ä¼¼æ¤œç´¢å˜ä¸€ã‚¯ã‚¨ãƒª",
                critical=True
            ),
            "integrated_workflow": BenchmarkTarget(
                operation_name="integrated_workflow",
                target_time_ms=600000, # 10åˆ†
                description="çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“",
                critical=True
            ),
            "fail_fast_response": BenchmarkTarget(
                operation_name="fail_fast_response",
                target_time_ms=100,    # 100ms
                description="Fail-fastã‚¨ãƒ©ãƒ¼æ¤œå‡º",
                critical=True
            ),
            "memory_usage_check": BenchmarkTarget(
                operation_name="memory_usage_check",
                target_time_ms=1536 * 1024 * 1024,  # 1.5GB (bytes)
                description="ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™",
                critical=True
            )
        }
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´
        self.metrics_history: List[PerformanceMetrics] = []
        self.max_history_size = int(os.getenv('PERFORMANCE_HISTORY_SIZE', '1000'))
    
    async def measure_performance(self, operation_name: str, coro_or_func, *args, **kwargs):
        """
        ã‚³ãƒ«ãƒ¼ãƒãƒ³ã¾ãŸã¯é–¢æ•°ã®æ€§èƒ½æ¸¬å®š
        
        Args:
            operation_name: æ“ä½œå
            coro_or_func: æ¸¬å®šå¯¾è±¡ã®ã‚³ãƒ«ãƒ¼ãƒãƒ³ã¾ãŸã¯é–¢æ•°
            *args, **kwargs: é–¢æ•°ã®å¼•æ•°
            
        Returns:
            (result, metrics): å®Ÿè¡Œçµæœã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        # åˆæœŸãƒªã‚½ãƒ¼ã‚¹æ¸¬å®š
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        initial_cpu = process.cpu_percent()
        
        start_time = time.perf_counter()
        start_timestamp = datetime.now()
        
        error_message = None
        result = None
        success = False
        
        try:
            # å®Ÿè¡Œ
            if asyncio.iscoroutinefunction(coro_or_func):
                result = await coro_or_func(*args, **kwargs)
            else:
                result = coro_or_func(*args, **kwargs)
            success = True
            
        except Exception as e:
            error_message = str(e)
            self.logger.warning(f"Performance measurement error in {operation_name}: {e}")
            
        finally:
            # çµ‚äº†ãƒªã‚½ãƒ¼ã‚¹æ¸¬å®š
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000  # ms
            
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            final_cpu = process.cpu_percent()
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ
            metrics = PerformanceMetrics(
                timestamp=start_timestamp,
                operation_name=operation_name,
                execution_time_ms=execution_time,
                memory_usage_mb=final_memory,
                cpu_usage_percent=final_cpu,
                success=success,
                error_message=error_message,
                additional_data={
                    'initial_memory_mb': initial_memory,
                    'memory_delta_mb': final_memory - initial_memory,
                    'cpu_delta_percent': final_cpu - initial_cpu
                }
            )
            
            # å±¥æ­´ã«è¿½åŠ 
            self.add_metrics(metrics)
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡
            self.evaluate_benchmark(metrics)
            
        return result, metrics
    
    def add_metrics(self, metrics: PerformanceMetrics):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã«è¿½åŠ """
        self.metrics_history.append(metrics)
        
        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(self.metrics_history) > self.max_history_size:
            self.metrics_history = self.metrics_history[-self.max_history_size:]
    
    def evaluate_benchmark(self, metrics: PerformanceMetrics):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ"""
        target = self.benchmark_targets.get(metrics.operation_name)
        if not target:
            return
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯ç‰¹åˆ¥æ‰±ã„
        if metrics.operation_name == "memory_usage_check":
            memory_bytes = metrics.memory_usage_mb * 1024 * 1024
            if memory_bytes > target.target_time_ms:  # target_time_msã‚’ãƒ¡ãƒ¢ãƒªåˆ¶é™ã¨ã—ã¦ä½¿ç”¨
                self.logger.warning(
                    f"ğŸ”´ Memory usage exceeded target: "
                    f"{metrics.memory_usage_mb:.1f}MB > {target.target_time_ms/1024/1024:.1f}MB"
                )
            else:
                self.logger.info(
                    f"âœ… Memory usage within target: "
                    f"{metrics.memory_usage_mb:.1f}MB < {target.target_time_ms/1024/1024:.1f}MB"
                )
            return
        
        # å®Ÿè¡Œæ™‚é–“è©•ä¾¡
        if metrics.execution_time_ms > target.target_time_ms:
            severity = "ğŸ”´ CRITICAL" if target.critical else "ğŸŸ¡ WARNING"
            self.logger.warning(
                f"{severity} Performance target missed: {metrics.operation_name} "
                f"{metrics.execution_time_ms:.1f}ms > {target.target_time_ms:.1f}ms target"
            )
        else:
            self.logger.info(
                f"âœ… Performance target achieved: {metrics.operation_name} "
                f"{metrics.execution_time_ms:.1f}ms < {target.target_time_ms:.1f}ms target"
            )
    
    def get_performance_report(self, 
                             operation_name: Optional[str] = None,
                             hours: int = 24) -> Dict[str, Any]:
        """æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_metrics = [
            m for m in self.metrics_history 
            if m.timestamp >= cutoff_time and (not operation_name or m.operation_name == operation_name)
        ]
        
        if not filtered_metrics:
            return {"message": "No metrics found for the specified criteria"}
        
        # çµ±è¨ˆè¨ˆç®—
        execution_times = [m.execution_time_ms for m in filtered_metrics if m.success]
        success_count = sum(1 for m in filtered_metrics if m.success)
        total_count = len(filtered_metrics)
        
        report = {
            "summary": {
                "total_operations": total_count,
                "successful_operations": success_count,
                "success_rate_percent": (success_count / total_count * 100) if total_count > 0 else 0,
                "time_range_hours": hours
            }
        }
        
        if execution_times:
            import statistics
            report["performance"] = {
                "avg_execution_time_ms": statistics.mean(execution_times),
                "median_execution_time_ms": statistics.median(execution_times),
                "min_execution_time_ms": min(execution_times),
                "max_execution_time_ms": max(execution_times),
                "std_deviation_ms": statistics.stdev(execution_times) if len(execution_times) > 1 else 0
            }
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ
        if operation_name and operation_name in self.benchmark_targets:
            target = self.benchmark_targets[operation_name]
            if execution_times:
                avg_time = statistics.mean(execution_times)
                report["benchmark"] = {
                    "target_ms": target.target_time_ms,
                    "current_avg_ms": avg_time,
                    "performance_ratio": avg_time / target.target_time_ms,
                    "target_achieved": avg_time <= target.target_time_ms
                }
        
        return report
    
    def save_benchmark_results(self, filename: Optional[str] = None):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’æ°¸ç¶šåŒ–"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"benchmark_results_{timestamp}.json"
        
        filepath = self.storage_path / filename
        
        # å…¨æ“ä½œã®ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        full_report = {
            "timestamp": datetime.now().isoformat(),
            "benchmark_targets": {name: asdict(target) for name, target in self.benchmark_targets.items()},
            "reports": {}
        }
        
        for operation_name in self.benchmark_targets.keys():
            full_report["reports"][operation_name] = self.get_performance_report(operation_name)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ğŸ“Š Benchmark results saved to {filepath}")
        return str(filepath)
    
    def load_benchmark_results(self, filename: str) -> Dict[str, Any]:
        """ä¿å­˜ã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã¿"""
        filepath = self.storage_path / filename
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.error(f"Benchmark file not found: {filepath}")
            return {}
        except json.JSONDecodeError as e:
            self.logger.error(f"Invalid JSON in benchmark file: {e}")
            return {}


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_global_monitor: Optional[PerformanceMonitor] = None

def get_performance_monitor() -> PerformanceMonitor:
    """ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¢ãƒ‹ã‚¿ãƒ¼å–å¾—"""
    global _global_monitor
    if _global_monitor is None:
        storage_path = os.getenv('BENCHMARK_RESULTS_PATH', './benchmarks/')
        _global_monitor = PerformanceMonitor(storage_path)
    return _global_monitor


# ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿é–¢æ•°
def monitor_performance(operation_name: str):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        async def wrapper(*args, **kwargs):
            monitor = get_performance_monitor()
            result, metrics = await monitor.measure_performance(operation_name, func, *args, **kwargs)
            return result
        return wrapper
    return decorator
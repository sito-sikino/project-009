"""
Improved Memory System with Production Enhancements
æœ¬ç•ªç’°å¢ƒå‘ã‘æ”¹å–„ç‰ˆMemory System
"""

import asyncio
import json
import logging
import gc
import time
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

import psutil

import redis.asyncio as redis
import asyncpg
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from pydantic import SecretStr

from ..config.settings import get_database_settings, get_ai_settings, get_system_settings


@dataclass
class MemoryItem:
    """Memory Item Data Structure"""
    content: str
    timestamp: datetime
    channel_id: str
    user_id: str
    agent: str
    confidence: float = 0.5
    metadata: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'content': self.content,
            'timestamp': self.timestamp.isoformat(),
            'channel_id': self.channel_id,
            'user_id': self.user_id,
            'agent': self.agent,
            'confidence': self.confidence,
            'metadata': self.metadata
        }


# Custom Exceptions
class MemorySystemError(Exception):
    """Memory SystemåŸºæœ¬ã‚¨ãƒ©ãƒ¼"""
    pass

class MemorySystemConnectionError(MemorySystemError):
    """æ¥ç¶šé–¢é€£ã‚¨ãƒ©ãƒ¼"""
    pass

class MemorySystemTransactionError(MemorySystemError):
    """ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–¢é€£ã‚¨ãƒ©ãƒ¼"""
    pass

class RedisConnectionError(MemorySystemConnectionError):
    """Redisæ¥ç¶šã‚¨ãƒ©ãƒ¼"""
    pass

class PostgreSQLConnectionError(MemorySystemConnectionError):
    """PostgreSQLæ¥ç¶šã‚¨ãƒ©ãƒ¼"""
    pass

class EmbeddingQuotaError(MemorySystemError):
    """Embedding APIåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    pass

class EmbeddingError(MemorySystemError):
    """Embeddingç”Ÿæˆã‚¨ãƒ©ãƒ¼"""
    pass


class RateLimiter:
    """API Rate Limiter"""
    def __init__(self, calls_per_second: float = 1.0):
        self.min_interval = 1.0 / calls_per_second
        self.last_called = 0.0
        self._lock = asyncio.Lock()
    
    async def acquire(self):
        async with self._lock:
            current_time = asyncio.get_event_loop().time()
            elapsed = current_time - self.last_called
            if elapsed < self.min_interval:
                await asyncio.sleep(self.min_interval - elapsed)
            self.last_called = asyncio.get_event_loop().time()


@dataclass
class HealthStatus:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    redis_connected: bool = False
    postgres_connected: bool = False
    embeddings_available: bool = False
    last_check: Optional[datetime] = None
    error_count: int = 0
    redis_pool_health: Optional[Dict[str, Any]] = None
    postgres_pool_health: Optional[Dict[str, Any]] = None
    performance_metrics: Optional[Dict[str, Any]] = None
    
    @property
    def is_healthy(self) -> bool:
        return all([self.redis_connected, self.postgres_connected, self.embeddings_available])


class ImprovedDiscordMemorySystem:
    """
    æœ¬ç•ªç’°å¢ƒå¯¾å¿œæ”¹å–„ç‰ˆMemory System
    
    æ”¹å–„ç‚¹:
    - ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
    - è©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    - ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
    - ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†
    - æ¥ç¶šãƒ—ãƒ¼ãƒªãƒ³ã‚°æœ€é©åŒ–
    """
    
    def __init__(self, 
                 redis_url: Optional[str] = None, 
                 postgres_url: Optional[str] = None,
                 gemini_api_key: Optional[str] = None):
        # ãƒ­ã‚®ãƒ³ã‚°è¨­å®šï¼ˆæœ€åˆã«åˆæœŸåŒ–ï¼‰
        self.logger = self._setup_logging()
        
        # è¨­å®šã‹ã‚‰å®‰å…¨ã«å–å¾—
        db_settings = get_database_settings()
        ai_settings = get_ai_settings()
        
        self.redis_url = redis_url or db_settings.redis_url
        postgres_url_env = postgres_url or db_settings.postgresql_url
        self.postgres_url = self._sanitize_postgres_url(postgres_url_env) if postgres_url_env else ''
        
        # Google Embeddingsè¨­å®š
        api_key = gemini_api_key or ai_settings.gemini_api_key
        self.embeddings_client = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=SecretStr(api_key) if api_key else None,
            task_type="RETRIEVAL_DOCUMENT"
        )
        
        # æ¥ç¶šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        self.redis: Optional[redis.Redis] = None
        self.redis_pool: Optional[redis.ConnectionPool] = None
        self.postgres_pool: Optional[asyncpg.Pool] = None
        
        # è¨­å®šï¼ˆä¸­å¤®ç®¡ç†å¯¾å¿œï¼‰
        self.hot_memory_limit = 20
        self.hot_memory_ttl = db_settings.hot_memory_ttl_seconds
        self.cold_retention_days = db_settings.cold_memory_retention_days
        self.migration_batch_size = db_settings.memory_migration_batch_size
        self.embedding_model = "models/text-embedding-004"
        self.similarity_threshold = 0.7
        self.max_cold_results = 10
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ (15 RPM = 0.25 RPS)
        self.embedding_rate_limiter = RateLimiter(calls_per_second=0.25)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        self.performance_metrics = {
            'hot_memory_operations': 0,
            'cold_memory_operations': 0,
            'embedding_generations': 0,
            'total_operations': 0
        }
        
        # ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        self.health_status = HealthStatus()
    
    def _sanitize_postgres_url(self, url: str) -> str:
        """PostgreSQL URL ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼ˆãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰éš è”½ï¼‰"""
        if not url:
            return 'postgresql://localhost:5432/discord_agent'
        
        # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰éƒ¨åˆ†ã‚’éš ã™
        if '@' in url and '://' in url:
            parts = url.split('@')
            if len(parts) == 2:
                cred_part = parts[0].split('://')[-1]
                if ':' in cred_part:
                    user = cred_part.split(':')[0]
                    sanitized = f"{parts[0].split('://')[0]}://{user}:****@{parts[1]}"
                    self.logger.info(f"PostgreSQL URL (sanitized): {sanitized}")
        
        return url
    
    def _setup_logging(self) -> logging.Logger:
        """æ§‹é€ åŒ–ãƒ­ã‚®ãƒ³ã‚°è¨­å®š"""
        logger = logging.getLogger(__name__)
        system_settings = get_system_settings()
        logger.setLevel(system_settings.log_level)
        
        # æ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        
        return logger
    
    async def initialize(self) -> bool:
        """Memory SystemåˆæœŸåŒ–ï¼ˆFail-fastå¯¾å¿œæ”¹å–„ç‰ˆï¼‰"""
        try:
            # è¨­å®šã‚’å–å¾—ï¼ˆãƒ¡ã‚½ãƒƒãƒ‰ã‚¹ã‚³ãƒ¼ãƒ—å†…ã§å®šç¾©ï¼‰
            db_settings = get_database_settings()
            
            # Redisæ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®šå¼·åŒ–ï¼ˆFail-fastå¯¾å¿œï¼‰
            if not self.redis_url:
                raise MemorySystemConnectionError("Redis URL not provided")
            
            # Redisæ¥ç¶šã®Fail-fastè¨­å®š
            self.redis_pool = redis.ConnectionPool.from_url(
                self.redis_url,
                max_connections=db_settings.redis_max_connections,
                decode_responses=True,
                encoding="utf-8",
                retry_on_timeout=False,  # Fail-fast: ãƒªãƒˆãƒ©ã‚¤ç„¡åŠ¹
                socket_timeout=1.0,      # Fail-fast: 1ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                socket_connect_timeout=1.0,  # Fail-fast: 1ç§’æ¥ç¶šã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                health_check_interval=30
            )
            self.redis = redis.Redis(connection_pool=self.redis_pool)
            
            # Redisæ¥ç¶šç¢ºèªï¼ˆFail-fastï¼‰
            redis_start = time.time()
            try:
                await asyncio.wait_for(self.redis.ping(), timeout=1.0)
                self.health_status.redis_connected = True
                self.logger.info(f"âœ… Redis connected in {time.time() - redis_start:.3f}s")
            except asyncio.TimeoutError:
                redis_error_time = time.time() - redis_start
                self.logger.error(f"âŒ Redis connection timeout after {redis_error_time:.3f}s")
                raise RedisConnectionError(f"Redis connection timeout after {redis_error_time:.3f}s")
            except Exception as e:
                redis_error_time = time.time() - redis_start
                self.logger.error(f"âŒ Redis initialization failed after {redis_error_time:.3f}s: {e}")
                raise RedisConnectionError(f"Redis initialization failed: {e}")
            
            # PostgreSQLæ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®šå¼·åŒ–ï¼ˆFail-fastå¯¾å¿œï¼‰
            postgres_start = time.time()
            try:
                self.postgres_pool = await asyncio.wait_for(
                    asyncpg.create_pool(
                        self.postgres_url,
                        min_size=db_settings.postgres_pool_min_size,
                        max_size=db_settings.postgres_pool_max_size,
                        max_queries=50000,
                        max_inactive_connection_lifetime=300.0,
                        command_timeout=2.0,  # Fail-fast: 2ç§’ã‚³ãƒãƒ³ãƒ‰ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                        server_settings={
                            'jit': 'off',
                            'application_name': 'discord_agent_memory_v2',
                            'statement_timeout': '2s',  # Fail-fast: 2ç§’ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                            'lock_timeout': '1s',       # Fail-fast: 1ç§’ãƒ­ãƒƒã‚¯ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                            'work_mem': '256MB',
                            'effective_cache_size': '1GB'
                        }
                    ),
                    timeout=3.0  # Fail-fast: 3ç§’ãƒ—ãƒ¼ãƒ«ä½œæˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                )
                
                # PostgreSQL & pgvectorç¢ºèªï¼ˆFail-fastï¼‰
                async with self.postgres_pool.acquire() as conn:
                    # pgvectoræ‹¡å¼µç¢ºèª
                    extensions = await asyncio.wait_for(
                        conn.fetch("SELECT extname FROM pg_extension WHERE extname = 'vector'"),
                        timeout=1.0
                    )
                    if not extensions:
                        raise PostgreSQLConnectionError("pgvector extension not found")
                    
                    await asyncio.wait_for(conn.fetchval('SELECT 1'), timeout=1.0)
                    self.health_status.postgres_connected = True
                
                postgres_time = time.time() - postgres_start
                self.logger.info(f"âœ… PostgreSQL connected in {postgres_time:.3f}s")
                
            except asyncio.TimeoutError:
                postgres_error_time = time.time() - postgres_start
                self.logger.error(f"âŒ PostgreSQL connection timeout after {postgres_error_time:.3f}s")
                raise PostgreSQLConnectionError(f"PostgreSQL connection timeout after {postgres_error_time:.3f}s")
            except Exception as e:
                postgres_error_time = time.time() - postgres_start
                self.logger.error(f"âŒ PostgreSQL initialization failed after {postgres_error_time:.3f}s: {e}")
                raise PostgreSQLConnectionError(f"PostgreSQL initialization failed: {e}")
            
            # Embeddings APIç¢ºèª
            self.health_status.embeddings_available = True
            
            # ãƒ˜ãƒ«ã‚¹ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            self.health_status.last_check = datetime.now()
            self.health_status.error_count = 0
            
            self.logger.info("ğŸ§  Discord Memory System initialized successfully")
            return True
            
        except redis.RedisError as e:
            self.logger.error(f"âŒ Redis initialization failed: {e}")
            raise RedisConnectionError(f"Redis connection failed: {e}")
            
        except asyncpg.PostgresError as e:
            self.logger.error(f"âŒ PostgreSQL initialization failed: {e}")
            raise PostgreSQLConnectionError(f"PostgreSQL connection failed: {e}")
            
        except (RedisConnectionError, PostgreSQLConnectionError) as e:
            # Fail-fast: æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¯å³åº§ã«å†ç™ºç”Ÿï¼ˆéš è”½ç¦æ­¢ï¼‰
            self.health_status.error_count += 1
            raise  # Fail-faståŸå‰‡: å³åº§ã«ã‚¨ãƒ©ãƒ¼ä¼æ’­
            
        except Exception as e:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã‚‚å³åº§ã«å ±å‘Š
            self.logger.error(f"âŒ Memory System initialization failed: {e}")
            self.health_status.error_count += 1
            raise MemorySystemError(f"Memory system initialization failed: {e}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type(redis.RedisError)
    )
    async def load_hot_memory(self, channel_id: str) -> List[Dict[str, Any]]:
        """Hot Memoryèª­ã¿è¾¼ã¿ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            if not self.redis:
                raise RedisConnectionError("Redis connection not available")
            
            redis_key = f"channel:{channel_id}:messages"
            
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½¿ç”¨ã§åŠ¹ç‡åŒ– + ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬
            start_time = asyncio.get_event_loop().time()
            if not self.redis:
                raise RedisConnectionError("Redis not connected")
            async with self.redis.pipeline(transaction=False) as pipe:
                pipe.lrange(redis_key, 0, self.hot_memory_limit - 1)
                pipe.expire(redis_key, self.hot_memory_ttl)
                pipe_results = await pipe.execute()
                results = pipe_results if pipe_results else []
            elapsed_time = asyncio.get_event_loop().time() - start_time
            
            messages = results[0] if results else []
            
            # JSON ãƒ‡ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
            hot_memory = []
            for msg_json in messages:
                try:
                    msg_data = json.loads(msg_json)
                    hot_memory.append(msg_data)
                except json.JSONDecodeError as e:
                    self.logger.warning(f"Invalid JSON in hot memory: {e}")
                    continue
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.performance_metrics['hot_memory_operations'] += 1
            
            self.logger.debug(
                f"Hot Memory loaded",
                extra={
                    "channel_id": channel_id, 
                    "message_count": len(hot_memory),
                    "load_time_ms": int(elapsed_time * 1000)
                }
            )
            return hot_memory
            
        except redis.RedisError as e:
            self.logger.error(f"Hot Memory load failed: {e}")
            self.health_status.redis_connected = False
            raise RedisConnectionError(f"Redis read failed: {e}")
    
    async def load_cold_memory(self, query: str, channel_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """Cold Memoryæ¤œç´¢ï¼ˆunified_memoriesãƒ†ãƒ¼ãƒ–ãƒ«ä½¿ç”¨ï¼‰"""
        try:
            if not self.postgres_pool:
                raise PostgreSQLConnectionError("PostgreSQL pool not available")
            
            # Embeddingç”Ÿæˆ
            query_embedding = await self.generate_embedding_with_rate_limit(query)
            if not query_embedding:
                raise EmbeddingError(f"Failed to generate embedding for query: {query}")
            
            # unified_memoriesãƒ†ãƒ¼ãƒ–ãƒ«æ¤œç´¢å®Ÿè¡Œï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰
            search_start = asyncio.get_event_loop().time()
            async with self.postgres_pool.acquire() as conn:
                # HNSWã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–è¨­å®š
                await conn.execute("SET hnsw.ef_search = 150")  # ãƒãƒ©ãƒ³ã‚¹å‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
                
                # æ¤œç´¢ã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼ˆæœ€é©åŒ–ç‰ˆï¼‰
                sql = """
                    SELECT memory_id, content, created_at, selected_agent, importance_score,
                           1 - (content_embedding <=> $1::vector) as similarity
                    FROM unified_memories 
                    WHERE ($2::text IS NULL OR channel_id = $2::bigint)
                      AND content_embedding IS NOT NULL
                    ORDER BY content_embedding <=> $1::vector
                    LIMIT $3
                """
                results = await conn.fetch(
                    sql, 
                    query_embedding, 
                    channel_id,
                    self.max_cold_results
                )
            search_time = asyncio.get_event_loop().time() - search_start
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.performance_metrics['cold_memory_operations'] += 1
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            cold_memory = []
            for row in results:
                cold_memory.append({
                    'memory_id': str(row['memory_id']),
                    'content': row['content'],
                    'similarity': float(row['similarity']) if row['similarity'] else 0.0,
                    'created_at': row['created_at'].isoformat() if row['created_at'] else '',
                    'selected_agent': row['selected_agent'] or 'unknown',
                    'importance_score': float(row['importance_score']) if row['importance_score'] else 0.0
                })
            
            self.logger.info(
                f"Cold Memory search completed",
                extra={
                    "query_length": len(query),
                    "results_count": len(cold_memory),
                    "channel_id": channel_id,
                    "search_time_ms": int(search_time * 1000)
                }
            )
            return cold_memory
            
        except asyncpg.PostgresError as e:
            self.logger.error(f"Cold Memory search failed: {e}")
            self.health_status.postgres_connected = False
            raise PostgreSQLConnectionError(f"PostgreSQL search failed: {e}")
        
        except Exception as e:
            self.logger.error(f"Cold Memory load failed: {e}")
            raise MemorySystemError(f"Cold memory search failed: {e}")
    
    async def migrate_to_cold_memory(self, channel_id: str, batch_size: Optional[int] = None) -> Dict[str, Any]:
        """ãƒ›ãƒƒãƒˆãƒ¡ãƒ¢ãƒªã‹ã‚‰ã‚³ãƒ¼ãƒ«ãƒ‰ãƒ¡ãƒ¢ãƒªã¸ã®ç§»è¡Œ - æœ€é©åŒ–ç‰ˆ"""
        if batch_size is None:
            batch_size = self.migration_batch_size
        
        start_time = asyncio.get_event_loop().time()
        memory_before = psutil.Process().memory_info().rss if psutil else 0
        
        try:
            if not self.redis or not self.postgres_pool:
                return {
                    'migrated_count': 0, 
                    'channel_id': channel_id, 
                    'status': 'failed',
                    'error': 'Redis or PostgreSQL not available'
                }
            
            redis_key = f"channel:{channel_id}:messages"
            
            # Redisã‹ã‚‰ãƒ›ãƒƒãƒˆãƒ¡ãƒ¢ãƒªèª­ã¿è¾¼ã¿
            messages_result = await self.redis.lrange(redis_key, 0, batch_size - 1)
            messages = messages_result if messages_result else []
            
            if not messages:
                return {
                    'migrated_count': 0,
                    'channel_id': channel_id,
                    'status': 'success',
                    'message': 'No messages to migrate'
                }
            
            migrated_count = 0
            failed_count = 0
            
            # å®Œå…¨ãªACIDãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè£…
            async with self.postgres_pool.acquire() as conn:
                async with conn.transaction():
                    for i, msg_json in enumerate(messages):
                        try:
                            msg_data = json.loads(msg_json)
                            content = msg_data.get('response_content', '')
                            
                            if content:
                                # Embeddingç”Ÿæˆï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»˜ãï¼‰
                                embedding = await self.generate_embedding_with_rate_limit(content)
                                if embedding:
                                    # unified_memoriesãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜
                                    await conn.execute("""
                                        INSERT INTO unified_memories (
                                            guild_id, channel_id, user_id, message_id,
                                            content, selected_agent, confidence,
                                            content_embedding, importance_score
                                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                                    """,
                                        0,  # guild_id
                                        int(channel_id) if channel_id.isdigit() else 0,
                                        0,  # user_id  
                                        0,  # message_id
                                        content,
                                        msg_data.get('selected_agent', 'unknown'),
                                        msg_data.get('confidence', 0.5),
                                        embedding,
                                        min(msg_data.get('confidence', 0.5), 1.0)
                                    )
                                    migrated_count += 1
                                else:
                                    failed_count += 1
                                    self.logger.warning(f"Failed to generate embedding for message {i}")
                            else:
                                failed_count += 1
                        
                        except (json.JSONDecodeError, asyncpg.PostgresError) as e:
                            failed_count += 1
                            self.logger.warning(f"Failed to migrate message {i}: {e}")
                            continue
                        
                        # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¤§é‡ãƒãƒƒãƒå‡¦ç†ã§ã®ãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
                        if i % 50 == 0 and i > 0:
                            gc.collect()
            
            # Rediså‰Šé™¤ï¼ˆç§»è¡ŒæˆåŠŸåˆ†ã®ã¿ï¼‰
            if migrated_count > 0:
                await self.redis.ltrim(redis_key, migrated_count, -1)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬
            elapsed_time = asyncio.get_event_loop().time() - start_time
            memory_after = psutil.Process().memory_info().rss if psutil else 0
            memory_delta = memory_after - memory_before
            
            self.logger.info(
                f"Migration completed for channel {channel_id}",
                extra={
                    "migrated_count": migrated_count,
                    "failed_count": failed_count,
                    "elapsed_ms": int(elapsed_time * 1000),
                    "memory_delta_mb": memory_delta / 1024 / 1024 if psutil else 0
                }
            )
            
            return {
                'migrated_count': migrated_count,
                'failed_count': failed_count,
                'channel_id': channel_id,
                'status': 'success',
                'elapsed_time': elapsed_time,
                'memory_delta_mb': memory_delta / 1024 / 1024 if psutil else 0
            }
            
        except (redis.RedisError, asyncpg.PostgresError) as e:
            self.logger.error(f"Migration connection error: {e}")
            raise MemorySystemConnectionError(f"Migration failed: {e}")
            
        except Exception as e:
            self.logger.error(f"Migration failed: {e}")
            return {
                'migrated_count': 0,
                'channel_id': channel_id,
                'status': 'failed',
                'error': str(e)
            }
    
    async def update_memory_transactional(self, conversation_data: Dict[str, Any]) -> bool:
        """Memoryæ›´æ–°ï¼ˆãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ä¿è¨¼ + ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ï¼‰"""
        start_time = asyncio.get_event_loop().time()
        
        try:
            channel_id = conversation_data.get('channel_id')
            if not channel_id:
                raise ValueError("channel_id is required")
            
            # Redisæ›´æ–°ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            memory_entry = {
                'timestamp': datetime.now().isoformat(),
                'messages': conversation_data.get('messages', []),
                'selected_agent': conversation_data.get('selected_agent'),
                'response_content': conversation_data.get('response_content'),
                'confidence': conversation_data.get('confidence', 0.5),
                'channel_id': channel_id
            }
            
            redis_key = f"channel:{channel_id}:messages"
            
            # PostgreSQLæ›´æ–°ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            messages = conversation_data.get('messages', [])
            if not messages:
                raise ValueError("No messages provided for memory update")
            
            latest_message = messages[-1] if messages else None
            user_content = getattr(latest_message, 'content', '') if latest_message else ''
            
            if not user_content:
                raise ValueError("No user content available for memory update")
            
            # Embeddingç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰
            content_embedding = await self._measure_performance(
                "content_embedding_generation",
                self.generate_embedding_with_rate_limit,
                user_content
            )
            response_embedding = await self._measure_performance(
                "response_embedding_generation", 
                self.generate_embedding_with_rate_limit,
                conversation_data.get('response_content', '')
            )
            
            # ã‚«ã‚¹ã‚¿ãƒ TTLå¯¾å¿œ
            ttl = conversation_data.get('custom_ttl', self.hot_memory_ttl)
            
            # åŸå­çš„ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            await self._execute_atomic_transaction(
                redis_key, memory_entry, ttl,
                conversation_data, user_content, content_embedding
            )
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°
            elapsed_time = asyncio.get_event_loop().time() - start_time
            self.logger.info(
                "Memory updated successfully",
                extra={
                    "channel_id": channel_id,
                    "elapsed_ms": int(elapsed_time * 1000),
                    "has_embedding": bool(content_embedding)
                }
            )
            return True
            
        except (redis.RedisError, asyncpg.PostgresError) as e:
            elapsed_time = asyncio.get_event_loop().time() - start_time
            self.logger.error(
                f"Memory update connection error: {e}",
                extra={"elapsed_ms": int(elapsed_time * 1000)}
            )
            raise MemorySystemConnectionError(f"Memory update failed: {e}")
            
        except Exception as e:
            elapsed_time = asyncio.get_event_loop().time() - start_time
            self.logger.error(
                f"Memory update failed: {e}",
                extra={"elapsed_ms": int(elapsed_time * 1000)}
            )
            return False
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((EmbeddingError, EmbeddingQuotaError))
    )
    async def generate_embedding_with_rate_limit(self, text: str) -> Optional[List[float]]:
        """Embeddingç”Ÿæˆï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒ»ãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        try:
            if not text.strip():
                return None
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            await self.embedding_rate_limiter.acquire()
            
            # ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œ
            max_chars = 2048 * 4
            truncated_text = text[:max_chars] if len(text) > max_chars else text
            
            # éåŒæœŸå®Ÿè¡Œ
            embedding_result = self.embeddings_client.embed_query(truncated_text)
            if asyncio.iscoroutine(embedding_result):
                embedding = await embedding_result
            else:
                embedding = embedding_result
            
            return embedding
            
        except Exception as e:
            error_str = str(e).lower()
            if "quota" in error_str or "rate" in error_str:
                self.logger.warning(f"Embedding API quota/rate limit: {e}")
                raise EmbeddingQuotaError(f"API quota exceeded: {e}")
            else:
                self.logger.error(f"Embedding generation failed: {e}")
                raise EmbeddingError(f"Embedding failed: {e}")
    
    async def get_detailed_health_status(self) -> Dict[str, Any]:
        """è©³ç´°ãªã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç›£è¦–ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        health_check_start = asyncio.get_event_loop().time()
        
        try:
            # Rediså¥åº·ç¢ºèª + ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®š
            redis_latency = None
            if self.redis:
                try:
                    redis_start = asyncio.get_event_loop().time()
                    await self.redis.ping()
                    redis_latency = asyncio.get_event_loop().time() - redis_start
                    self.health_status.redis_connected = True
                    
                    # Redisãƒ—ãƒ¼ãƒ«çµ±è¨ˆ
                    if self.redis_pool:
                        pool_info = {
                            'max_connections': self.redis_pool.max_connections,
                            'created_connections': len(self.redis_pool._created_connections) if hasattr(self.redis_pool, '_created_connections') else 'unknown'
                        }
                    else:
                        pool_info = {'error': 'pool not available'}
                    
                    self.health_status.redis_pool_health = pool_info
                    
                except Exception as e:
                    self.health_status.redis_connected = False
                    self.health_status.redis_pool_health = {'error': str(e)}
            
            # PostgreSQLå¥åº·ç¢ºèª + ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®š
            postgres_latency = None
            if self.postgres_pool:
                try:
                    postgres_start = asyncio.get_event_loop().time()
                    async with self.postgres_pool.acquire() as conn:
                        await conn.fetchval('SELECT 1')
                    postgres_latency = asyncio.get_event_loop().time() - postgres_start
                    self.health_status.postgres_connected = True
                    
                    # PostgreSQLãƒ—ãƒ¼ãƒ«çµ±è¨ˆ
                    pool_info = {
                        'min_size': self.postgres_pool._minsize,
                        'max_size': self.postgres_pool._maxsize,
                        'current_size': self.postgres_pool.get_size(),
                        'idle_connections': self.postgres_pool.get_idle_size()
                    }
                    self.health_status.postgres_pool_health = pool_info
                    
                except Exception as e:
                    self.health_status.postgres_connected = False
                    self.health_status.postgres_pool_health = {'error': str(e)}
            
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨çŠ¶æ³
            resource_info = {}
            if psutil:
                process = psutil.Process()
                resource_info = {
                    'memory_mb': process.memory_info().rss / 1024 / 1024,
                    'cpu_percent': process.cpu_percent(),
                    'open_files': len(process.open_files()) if hasattr(process, 'open_files') else 'unknown'
                }
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            performance_metrics = {
                'redis_latency_ms': int(redis_latency * 1000) if redis_latency else None,
                'postgres_latency_ms': int(postgres_latency * 1000) if postgres_latency else None,
                'health_check_time_ms': int((asyncio.get_event_loop().time() - health_check_start) * 1000)
            }
            
            self.health_status.performance_metrics = performance_metrics
            self.health_status.last_check = datetime.now()
            
            return {
                "status": "healthy" if self.health_status.is_healthy else "unhealthy",
                "components": {
                    "redis": {
                        "connected": self.health_status.redis_connected,
                        "latency_ms": performance_metrics['redis_latency_ms'],
                        "pool": self.health_status.redis_pool_health
                    },
                    "postgres": {
                        "connected": self.health_status.postgres_connected,
                        "latency_ms": performance_metrics['postgres_latency_ms'],
                        "pool": self.health_status.postgres_pool_health
                    },
                    "embeddings": {
                        "available": self.health_status.embeddings_available
                    }
                },
                "resources": resource_info,
                "performance": performance_metrics,
                "last_check": self.health_status.last_check.isoformat(),
                "error_count": self.health_status.error_count
            }
            
        except Exception as e:
            self.logger.error(f"Health check failed: {e}")
            return {
                "status": "error",
                "error": str(e),
                "last_check": datetime.now().isoformat()
            }
    
    async def get_health_status(self) -> Dict[str, Any]:
        """ç°¡æ˜“ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
        detailed = await self.get_detailed_health_status()
        return {
            "status": detailed["status"],
            "redis": detailed["components"]["redis"]["connected"],
            "postgres": detailed["components"]["postgres"]["connected"],
            "embeddings": detailed["components"]["embeddings"]["available"],
            "last_check": detailed["last_check"],
            "error_count": detailed["error_count"]
        }
    
    async def store_task(self, task_key: str, task_data: Dict[str, Any]) -> bool:
        """ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’Redisã«ä¿å­˜"""
        if not self.redis:
            raise RedisConnectionError("Redis not available for task storage")
                
        task_json = json.dumps(task_data, ensure_ascii=False, default=str)
        await self.redis.setex(task_key, 86400, task_json)  # 24æ™‚é–“ä¿æŒ
        self.logger.info(f"Task stored: {task_key}")
        return True
    
    async def cleanup(self) -> None:
        """ãƒªã‚½ãƒ¼ã‚¹æ­£å¸¸çµ‚äº†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        cleanup_tasks = []
        
        try:
            # Redisã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.redis:
                cleanup_tasks.append(self._cleanup_redis())
            
            # PostgreSQLã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.postgres_pool:
                cleanup_tasks.append(self._cleanup_postgres())
            
            # ä¸¦åˆ—ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
            if cleanup_tasks:
                await asyncio.gather(*cleanup_tasks, return_exceptions=True)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            gc.collect()
            
            self.logger.info("Memory System cleanup completed successfully")
                
        except Exception as e:
            self.logger.error(f"Cleanup failed: {e}")
    
    async def _cleanup_redis(self) -> None:
        """é©åˆ‡ãªRedisã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.redis:
                await self.redis.close()
            if self.redis_pool:
                await self.redis_pool.disconnect()
            self.logger.info("Redis connection closed")
        except Exception as e:
            self.logger.warning(f"Redis cleanup warning: {e}")
    
    async def _cleanup_postgres(self) -> None:
        """é©åˆ‡ãªPostgreSQLã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.postgres_pool:
                await self.postgres_pool.close()
            self.logger.info("PostgreSQL pool closed")
        except Exception as e:
            self.logger.warning(f"PostgreSQL cleanup warning: {e}")


    # æ–°ã—ã„ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    async def _measure_performance(self, operation_name: str, func, *args, **kwargs):
        """å„æ“ä½œã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®š"""
        start_time = asyncio.get_event_loop().time()
        memory_before = psutil.Process().memory_info().rss if psutil else 0
        
        try:
            result = await func(*args, **kwargs)
            success = True
        finally:
            elapsed_time = asyncio.get_event_loop().time() - start_time
            memory_after = psutil.Process().memory_info().rss if psutil else 0
            memory_delta = memory_after - memory_before
            
            self.logger.info(f"Performance: {operation_name}", extra={
                'elapsed_ms': int(elapsed_time * 1000),
                'memory_delta_mb': memory_delta / 1024 / 1024 if psutil else 0,
                'success': success
            })
        
        return result
    
    # æˆ¦ç•¥çš„ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Œå…¨å‰Šé™¤ï¼ˆFail-faståŸå‰‡é©ç”¨ï¼‰
    
    async def _process_large_batch(self, items: List[Any], batch_size: int = 100):
        """å¤§ããªãƒãƒƒãƒå‡¦ç†ã®åˆ†å‰²å‡¦ç†"""
        results = []
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_result = await self._process_batch(batch)
            results.extend(batch_result)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if i % (batch_size * 10) == 0:
                gc.collect()
        
        return results
    
    async def _process_batch(self, batch: List[Any]):
        """ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        return batch  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè£…
    
    async def _update_redis_only(self, redis_key: str, memory_entry: Dict[str, Any], conversation_data: Dict[str, Any]) -> bool:
        """é ‚ç´šRedisã®ã¿æ›´æ–°å‡¦ç†"""
        try:
            if not self.redis:
                return False
            ttl = conversation_data.get('custom_ttl', self.hot_memory_ttl)
            async with self.redis.pipeline(transaction=True) as pipe:
                pipe.lpush(redis_key, json.dumps(memory_entry, default=str))
                pipe.ltrim(redis_key, 0, self.hot_memory_limit - 1)
                pipe.expire(redis_key, ttl)
                await pipe.execute()
            return True
        except Exception as e:
            self.logger.error(f"Redis-only update failed: {e}")
            return False
    
    async def _execute_atomic_transaction(self, redis_key: str, memory_entry: Dict[str, Any], 
                                         ttl: int, conversation_data: Dict[str, Any], 
                                         user_content: str, content_embedding: List[float]) -> None:
        """åŸå­çš„ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        # Redisæ›´æ–°
        if not self.redis:
            raise RedisConnectionError("Redis not connected")
        async with self.redis.pipeline(transaction=True) as pipe:
            pipe.lpush(redis_key, json.dumps(memory_entry, default=str))
            pipe.ltrim(redis_key, 0, self.hot_memory_limit - 1)
            pipe.expire(redis_key, ttl)
            await pipe.execute()
        
        # PostgreSQLæ›´æ–°ï¼ˆEmbeddingãŒç”Ÿæˆã§ããŸå ´åˆã®ã¿ï¼‰
        if content_embedding and self.postgres_pool:
            async with self.postgres_pool.acquire() as conn:
                async with conn.transaction():
                    await conn.execute("""
                        INSERT INTO unified_memories (
                            guild_id, channel_id, user_id, message_id,
                            content, selected_agent, confidence,
                            content_embedding, importance_score
                        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    """,
                        int(conversation_data.get('guild_id', 0)),
                        int(conversation_data.get('channel_id', 0)) if str(conversation_data.get('channel_id', '')).isdigit() else 0,
                        int(conversation_data.get('user_id', 0)) if str(conversation_data.get('user_id', 0)).isdigit() else 0,
                        int(conversation_data.get('message_id', 0)) if str(conversation_data.get('message_id', 0)).isdigit() else 0,
                        user_content,
                        conversation_data.get('selected_agent', 'unknown'),
                        conversation_data.get('confidence', 0.5),
                        content_embedding,
                        min(conversation_data.get('confidence', 0.5), 1.0)
                    )


# Factoryé–¢æ•°
def create_improved_memory_system() -> ImprovedDiscordMemorySystem:
    """æ”¹å–„ç‰ˆMemory Systemç”Ÿæˆ"""
    # ä¸­å¤®è¨­å®šã‹ã‚‰å–å¾—
    db_settings = get_database_settings()
    ai_settings = get_ai_settings()
    
    redis_url = db_settings.redis_url
    postgres_url = db_settings.postgresql_url
    gemini_api_key = ai_settings.gemini_api_key
    
    return ImprovedDiscordMemorySystem(
        redis_url=redis_url,
        postgres_url=postgres_url,
        gemini_api_key=gemini_api_key
    )
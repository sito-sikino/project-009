"""
Long-term Memory System with 3-API Batch Processing
3-APIè¨­è¨ˆã«ã‚ˆã‚‹é•·æœŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¾¡å€¤åˆ¤å®šãƒ»é‡è¤‡æ¤œå‡ºãƒ»embeddingç”Ÿæˆï¼‰
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path

import redis.asyncio as redis
import asyncpg
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings

from .deduplication_system import MinHashDeduplicator, MemoryItem, create_memory_item_from_dict
from .memory_system import ImprovedDiscordMemorySystem


@dataclass
class ProcessedMemory:
    """å‡¦ç†æ¸ˆã¿è¨˜æ†¶ï¼ˆ3-APIå‡¦ç†å¾Œï¼‰"""
    id: str
    original_content: str
    structured_content: str
    timestamp: datetime
    channel_id: int
    user_id: str
    memory_type: str
    entities: List[Dict[str, str]]  # æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
    importance_score: float
    progress_indicators: Dict[str, Any]
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    minhash_signature: Optional[bytes] = None


@dataclass
class ProgressDifferential:
    """é€²æ—å·®åˆ†ï¼ˆå‰æ—¥æ¯”è¼ƒçµæœï¼‰"""
    date: datetime
    new_entities: List[str]
    progressed_entities: List[str]
    stagnant_entities: List[str]
    completed_tasks: List[str]
    new_skills: List[str]
    overall_summary: str


class LongTermMemoryProcessor:
    """3-APIé•·æœŸè¨˜æ†¶å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self,
                 redis_url: str = None,
                 postgres_url: str = None,
                 gemini_api_key: str = None):
        
        self.logger = logging.getLogger(__name__)
        
        # åŸºç›¤ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ 
        self.memory_system = ImprovedDiscordMemorySystem(
            redis_url=redis_url,
            postgres_url=postgres_url,
            gemini_api_key=gemini_api_key
        )
        
        # Gemini 2.0 Flashï¼ˆçµ±åˆåˆ†æç”¨ï¼‰
        self.gemini_flash = GoogleGenerativeAI(
            model="models/gemini-2.0-flash-exp",
            google_api_key=gemini_api_key,
            temperature=0.1
        )
        
        # Embedding client
        self.embeddings_client = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=gemini_api_key
        )
        
        # é‡è¤‡æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
        self.deduplicator = MinHashDeduplicator(
            threshold=0.8,  # é«˜ç²¾åº¦é‡è¤‡æ¤œå‡º
            num_perm=128
        )
        
        # APIä½¿ç”¨é‡ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼ˆ1æ—¥3å›åˆ¶é™ï¼‰
        self.api_usage_count = 0
        self.last_processing_date = None
        
    async def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        await self.memory_system.initialize()
        self.logger.info("Long-term Memory Processor initialized")
    
    async def daily_memory_consolidation(self, 
                                       target_date: datetime = None) -> Tuple[List[ProcessedMemory], ProgressDifferential]:
        """
        æ—¥æ¬¡è¨˜æ†¶çµ±åˆå‡¦ç†ï¼ˆ3-APIï¼‰
        
        Returns:
            (å‡¦ç†æ¸ˆã¿è¨˜æ†¶ä¸€è¦§, é€²æ—å·®åˆ†)
        """
        if target_date is None:
            target_date = datetime.now()
        
        # APIä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        if not self._check_api_quota(target_date):
            raise RuntimeError("APIä½¿ç”¨é‡åˆ¶é™è¶…éï¼ˆ1æ—¥3å›åˆ¶é™ï¼‰")
        
        self.logger.info(f"ğŸ§  é•·æœŸè¨˜æ†¶åŒ–å‡¦ç†é–‹å§‹: {target_date.strftime('%Y-%m-%d')}")
        start_time = datetime.now()
        
        try:
            # 1. Redisã‹ã‚‰ã®è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿å–å¾—
            raw_memories = await self._fetch_daily_memories(target_date)
            self.logger.info(f"ğŸ“Š å–å¾—è¨˜æ†¶æ•°: {len(raw_memories)}ä»¶")
            
            if not raw_memories:
                self.logger.warning("å‡¦ç†å¯¾è±¡è¨˜æ†¶ãªã—")
                return [], ProgressDifferential(
                    date=target_date,
                    new_entities=[], progressed_entities=[], stagnant_entities=[],
                    completed_tasks=[], new_skills=[],
                    overall_summary="å‡¦ç†å¯¾è±¡è¨˜æ†¶ãªã—"
                )
            
            # 2. API 1: Gemini 2.0 Flashçµ±åˆåˆ†æ
            processed_memories = await self._api1_unified_analysis(raw_memories)
            self.api_usage_count += 1
            self.logger.info(f"âœ… API 1å®Œäº†: {len(processed_memories)}ä»¶å‡¦ç†")
            
            # 3. MinHashé‡è¤‡é™¤å»ï¼ˆAPIä¸è¦ï¼‰
            unique_memories = self._remove_duplicates(processed_memories)
            duplicate_ratio = (len(processed_memories) - len(unique_memories)) / len(processed_memories) * 100
            self.logger.info(f"ğŸ” é‡è¤‡é™¤å»: {duplicate_ratio:.1f}% ({len(unique_memories)}ä»¶æ®‹å­˜)")
            
            # 4. API 2: é€²æ—å·®åˆ†ç”Ÿæˆ
            progress_diff = await self._api2_progress_differential(unique_memories, target_date)
            self.api_usage_count += 1
            self.logger.info("âœ… API 2å®Œäº†: é€²æ—å·®åˆ†ç”Ÿæˆ")
            
            # 5. API 3: ãƒãƒƒãƒembeddingç”Ÿæˆ
            final_memories = await self._api3_batch_embeddings(unique_memories)
            self.api_usage_count += 1
            self.logger.info(f"âœ… API 3å®Œäº†: {len(final_memories)}ä»¶embeddingç”Ÿæˆ")
            
            # 6. PostgreSQLä¿å­˜
            saved_count = await self._store_unified_memories(final_memories)
            self.logger.info(f"ğŸ’¾ PostgreSQLä¿å­˜: {saved_count}ä»¶")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"ğŸ‰ é•·æœŸè¨˜æ†¶åŒ–å®Œäº†: {processing_time:.1f}ç§’, APIä½¿ç”¨é‡: 3å›")
            
            return final_memories, progress_diff
            
        except Exception as e:
            self.logger.error(f"âŒ é•·æœŸè¨˜æ†¶åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _fetch_daily_memories(self, target_date: datetime) -> List[Dict[str, Any]]:
        """Redis ã‹ã‚‰æŒ‡å®šæ—¥ã®è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        date_key = target_date.strftime('%Y-%m-%d')
        
        # Redisæ¥ç¶š
        redis_client = redis.from_url(self.memory_system.redis_url)
        
        # æ—¥æ¬¡è¨˜æ†¶ã‚­ãƒ¼å–å¾—
        pattern = f"memory:{date_key}:*"
        keys = await redis_client.keys(pattern)
        
        memories = []
        for key in keys:
            data = await redis_client.hgetall(key)
            if data:
                # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                decoded_data = {k.decode(): v.decode() for k, v in data.items()}
                memories.append(decoded_data)
        
        await redis_client.close()
        return memories
    
    async def _api1_unified_analysis(self, raw_memories: List[Dict[str, Any]]) -> List[ProcessedMemory]:
        """API 1: Gemini 2.0 Flashçµ±åˆåˆ†æ"""
        self.logger.info("ğŸ” API 1: Gemini 2.0 Flashçµ±åˆåˆ†æé–‹å§‹")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        analysis_prompt = self._build_unified_analysis_prompt(raw_memories)
        
        # Gemini 2.0 Flashå®Ÿè¡Œ
        response = await self.gemini_flash.ainvoke(analysis_prompt)
        
        # å¿œç­”ãƒ‘ãƒ¼ã‚¹
        processed_memories = self._parse_analysis_response(response, raw_memories)
        
        return processed_memories
    
    def _build_unified_analysis_prompt(self, raw_memories: List[Dict[str, Any]]) -> str:
        """çµ±åˆåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        memories_json = json.dumps(raw_memories, ensure_ascii=False, indent=2)
        
        return f"""
# Discordè¨˜æ†¶çµ±åˆåˆ†æã‚¿ã‚¹ã‚¯

## å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
ä»¥ä¸‹ã¯1æ—¥åˆ†ã®Discordä¼šè©±è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ã§ã™ï¼š

```json
{memories_json}
```

## åˆ†æè¦æ±‚

å„è¨˜æ†¶ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

### 1. ä¾¡å€¤åˆ¤å®šï¼ˆ0.0-1.0ï¼‰
- 0.8-1.0: é‡è¦ï¼ˆæŠ€è¡“ç¿’å¾—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²å±•ã€é‡è¦æ±ºå®šï¼‰
- 0.5-0.7: ä¸­ç¨‹åº¦ï¼ˆæ—¥å¸¸ä½œæ¥­ã€è­°è«–ã€ã‚¢ã‚¤ãƒ‡ã‚¢ï¼‰
- 0.0-0.4: ä½ï¼ˆé›‘è«‡ã€é‡è¤‡æƒ…å ±ï¼‰

### 2. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
æŠ€è¡“ã€äººç‰©ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã‚¹ã‚­ãƒ«ã€ãƒ„ãƒ¼ãƒ«ç­‰ã‚’æŠ½å‡º
ä¾‹: {{"name": "TypeScript", "type": "technology"}}

### 3. è¨˜æ†¶ã‚¿ã‚¤ãƒ—åˆ†é¡
- conversation: é€šå¸¸ä¼šè©±
- task: ã‚¿ã‚¹ã‚¯é–¢é€£
- progress: é€²æ—å ±å‘Š
- learning: å­¦ç¿’è¨˜éŒ²
- decision: æ„æ€æ±ºå®š

### 4. æ§‹é€ åŒ–
å†…å®¹ã‚’ç°¡æ½”ã§æ¤œç´¢ã—ã‚„ã™ã„å½¢å¼ã«æ§‹é€ åŒ–

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
```json
[
  {{
    "id": "è¨˜æ†¶ID",
    "structured_content": "æ§‹é€ åŒ–ã•ã‚ŒãŸå†…å®¹",
    "memory_type": "åˆ†é¡",
    "entities": [
      {{"name": "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å", "type": "ã‚¿ã‚¤ãƒ—"}}
    ],
    "importance_score": 0.8,
    "progress_indicators": {{
      "skill_development": "ç¿’å¾—æŠ€è¡“",
      "project_advancement": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²å±•",
      "problem_resolution": "è§£æ±ºèª²é¡Œ"
    }}
  }}
]
```

é‡è¦æ€§0.4æœªæº€ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºã¯åŒ…æ‹¬çš„ã«è¡Œã„ã€å€‹åˆ¥å…·ä½“çš„è¦ç´ ï¼ˆTypeScriptã€è»¢è·ç­‰ï¼‰ãŒæ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
"""
    
    def _parse_analysis_response(self, response: str, raw_memories: List[Dict[str, Any]]) -> List[ProcessedMemory]:
        """åˆ†æå¿œç­”ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONéƒ¨åˆ†æŠ½å‡º
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            json_content = response[json_start:json_end]
            
            analysis_results = json.loads(json_content)
            
            # ProcessedMemoryä½œæˆ
            processed_memories = []
            for result in analysis_results:
                # å…ƒè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
                original_memory = next(
                    (m for m in raw_memories if m.get('id') == result['id']),
                    None
                )
                
                if original_memory:
                    processed_memory = ProcessedMemory(
                        id=result['id'],
                        original_content=original_memory.get('content', ''),
                        structured_content=result['structured_content'],
                        timestamp=datetime.fromisoformat(original_memory.get('timestamp', datetime.now().isoformat())),
                        channel_id=int(original_memory.get('channel_id', 0)),
                        user_id=original_memory.get('user_id', ''),
                        memory_type=result['memory_type'],
                        entities=result['entities'],
                        importance_score=result['importance_score'],
                        progress_indicators=result.get('progress_indicators', {}),
                        metadata=original_memory.get('metadata', {})
                    )
                    processed_memories.append(processed_memory)
            
            return processed_memories
            
        except Exception as e:
            self.logger.error(f"åˆ†æå¿œç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _remove_duplicates(self, processed_memories: List[ProcessedMemory]) -> List[ProcessedMemory]:
        """MinHashé‡è¤‡é™¤å»"""
        memory_items = []
        for memory in processed_memories:
            item = MemoryItem(
                id=memory.id,
                content=memory.structured_content,
                timestamp=memory.timestamp,
                channel_id=memory.channel_id,
                user_id=memory.user_id,
                memory_type=memory.memory_type,
                metadata=memory.metadata
            )
            memory_items.append(item)
        
        # é‡è¤‡é™¤å»å®Ÿè¡Œ
        unique_items = self.deduplicator.batch_deduplicate(memory_items)
        
        # ProcessedMemoryå½¢å¼ã«æˆ»ã™
        unique_ids = {item.id for item in unique_items}
        unique_memories = [m for m in processed_memories if m.id in unique_ids]
        
        # MinHashã‚·ã‚°ãƒãƒãƒ£è¿½åŠ 
        signatures = self.deduplicator.export_minhash_signatures()
        for memory in unique_memories:
            memory.minhash_signature = signatures.get(memory.id)
        
        return unique_memories
    
    async def _api2_progress_differential(self, 
                                        memories: List[ProcessedMemory], 
                                        target_date: datetime) -> ProgressDifferential:
        """API 2: é€²æ—å·®åˆ†ç”Ÿæˆ"""
        self.logger.info("ğŸ“ˆ API 2: é€²æ—å·®åˆ†ç”Ÿæˆé–‹å§‹")
        
        # å‰æ—¥è¨˜æ†¶å–å¾—
        previous_date = target_date - timedelta(days=1)
        previous_snapshot = await self._get_previous_memory_snapshot(previous_date)
        
        # å·®åˆ†åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        diff_prompt = self._build_progress_diff_prompt(memories, previous_snapshot, target_date)
        
        # Geminiå®Ÿè¡Œ
        response = await self.gemini_flash.ainvoke(diff_prompt)
        
        # å·®åˆ†çµæœãƒ‘ãƒ¼ã‚¹
        progress_diff = self._parse_progress_diff_response(response, target_date)
        
        return progress_diff
    
    async def _api3_batch_embeddings(self, memories: List[ProcessedMemory]) -> List[ProcessedMemory]:
        """API 3: ãƒãƒƒãƒembeddingç”Ÿæˆ"""
        self.logger.info("ğŸ”— API 3: ãƒãƒƒãƒembeddingç”Ÿæˆé–‹å§‹")
        
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
        texts = [memory.structured_content for memory in memories]
        
        # ãƒãƒƒãƒembeddingç”Ÿæˆ
        embeddings = await self.embeddings_client.aembed_documents(texts)
        
        # embeddingå‰²ã‚Šå½“ã¦
        for memory, embedding in zip(memories, embeddings):
            memory.embedding = embedding
        
        return memories
    
    async def _store_unified_memories(self, memories: List[ProcessedMemory]) -> int:
        """PostgreSQLçµ±ä¸€è¨˜æ†¶ä¿å­˜"""
        if not memories:
            return 0
        
        # PostgreSQLæ¥ç¶š
        conn = await asyncpg.connect(self.memory_system.postgres_url)
        
        try:
            saved_count = 0
            for memory in memories:
                await conn.execute("""
                    INSERT INTO unified_memories (
                        id, timestamp, channel_id, user_id, message_id,
                        content, memory_type, metadata, embedding,
                        minhash_signature, entities, progress_type, importance_score
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    ON CONFLICT (id) DO NOTHING
                """, 
                    memory.id,
                    memory.timestamp,
                    memory.channel_id,
                    memory.user_id,
                    None,  # message_id
                    memory.structured_content,
                    memory.memory_type,
                    json.dumps(memory.metadata),
                    memory.embedding,
                    memory.minhash_signature,
                    json.dumps(memory.entities),
                    memory.progress_indicators.get('project_advancement', ''),
                    memory.importance_score
                )
                saved_count += 1
            
            return saved_count
            
        finally:
            await conn.close()
    
    def _check_api_quota(self, target_date: datetime) -> bool:
        """APIä½¿ç”¨é‡åˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ1æ—¥3å›ï¼‰"""
        current_date = target_date.date()
        
        if self.last_processing_date != current_date:
            # æ–°ã—ã„æ—¥ - ãƒªã‚»ãƒƒãƒˆ
            self.api_usage_count = 0
            self.last_processing_date = current_date
            return True
        
        return self.api_usage_count < 3
    
    # ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆprogress_diffé–¢é€£ï¼‰ã¯ç°¡ç•¥åŒ–ã—ã¦å¾Œã§å®Ÿè£…
    async def _get_previous_memory_snapshot(self, date: datetime) -> Dict[str, Any]:
        """å‰æ—¥è¨˜æ†¶ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        return {"entities": [], "summary": "å‰æ—¥ãƒ‡ãƒ¼ã‚¿ãªã—"}
    
    def _build_progress_diff_prompt(self, memories, previous_snapshot, target_date) -> str:
        """é€²æ—å·®åˆ†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        return f"é€²æ—å·®åˆ†ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚æ—¥ä»˜: {target_date.strftime('%Y-%m-%d')}"
    
    def _parse_progress_diff_response(self, response: str, target_date: datetime) -> ProgressDifferential:
        """é€²æ—å·®åˆ†å¿œç­”ãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        return ProgressDifferential(
            date=target_date,
            new_entities=[],
            progressed_entities=[],
            stagnant_entities=[],
            completed_tasks=[],
            new_skills=[],
            overall_summary="é€²æ—å·®åˆ†å‡¦ç†å®Œäº†"
        )
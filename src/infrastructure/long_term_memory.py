"""
Long-term Memory System with 3-API Batch Processing
3-APIè¨­è¨ˆã«ã‚ˆã‚‹é•·æœŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ï¼ˆä¾¡å€¤åˆ¤å®šãƒ»é‡è¤‡æ¤œå‡ºãƒ»embeddingç”Ÿæˆï¼‰
"""

import asyncio
import json
import logging
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass

import psutil

import redis.asyncio as redis
import asyncpg
from langchain_google_genai import GoogleGenerativeAI
from pydantic import SecretStr

from .deduplication_system import MinHashDeduplicator, MemoryItem
from .memory_system import (
    ImprovedDiscordMemorySystem,
    MemorySystemError,
    MemorySystemConnectionError
)
from .embedding_client import GoogleEmbeddingClient


@dataclass
class ProcessedMemory:
    """å‡¦ç†æ¸ˆã¿è¨˜æ†¶ï¼ˆ3-APIå‡¦ç†å¾Œï¼‰"""
    id: str
    original_content: str
    structured_content: str
    timestamp: datetime
    channel_id: int
    user_id: str
    memory_type: str
    entities: List[Dict[str, str]]  # æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
    importance_score: float
    progress_indicators: Dict[str, Any]
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    minhash_signature: Optional[bytes] = None


@dataclass
class ProgressDifferential:
    """é€²æ—å·®åˆ†ï¼ˆå‰æ—¥æ¯”è¼ƒçµæœï¼‰"""
    date: datetime
    new_entities: List[str]
    progressed_entities: List[str]
    stagnant_entities: List[str]
    completed_tasks: List[str]
    new_skills: List[str]
    overall_summary: str


class LongTermMemoryProcessor:
    """3-APIé•·æœŸè¨˜æ†¶å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self,
                 redis_url: Optional[str] = None,
                 postgres_url: Optional[str] = None,
                 gemini_api_key: Optional[str] = None):

        self.logger = logging.getLogger(__name__)

        # Phase 3: ç’°å¢ƒå¤‰æ•°åˆ¶å¾¡æ©Ÿèƒ½å®Ÿè£…
        self.is_enabled = os.getenv('LONG_TERM_MEMORY_ENABLED', 'false').lower() == 'true'
        self.logger.info(f"ğŸ§  Long-term Memory System: {'ENABLED' if self.is_enabled else 'DISABLED'}")

        # åŸºç›¤ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ 
        self.memory_system = ImprovedDiscordMemorySystem(
            redis_url=redis_url,
            postgres_url=postgres_url,
            gemini_api_key=gemini_api_key
        )

        # Gemini 2.0 Flashï¼ˆçµ±åˆåˆ†æç”¨ï¼‰
        api_key = gemini_api_key or os.getenv('GEMINI_API_KEY')
        self.gemini_flash = GoogleGenerativeAI(
            model="models/gemini-2.0-flash-exp",
            api_key=SecretStr(api_key) if api_key else None,
            temperature=0.1
        )

        # GoogleEmbeddingClientï¼ˆPhase 1çµ±åˆï¼‰
        self.embeddings_client = GoogleEmbeddingClient(
            api_key=gemini_api_key,
            task_type="RETRIEVAL_DOCUMENT"
        )

        # é‡è¤‡æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆç’°å¢ƒå¤‰æ•°åˆ¶å¾¡ï¼‰
        deduplication_threshold = float(os.getenv('DEDUPLICATION_THRESHOLD', '0.8'))
        self.deduplicator = MinHashDeduplicator(
            threshold=deduplication_threshold,
            num_perm=128
        )

        # APIä½¿ç”¨é‡ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ï¼ˆç’°å¢ƒå¤‰æ•°åˆ¶å¾¡ï¼‰
        self.daily_api_limit = int(os.getenv('API_QUOTA_DAILY_LIMIT', '3'))
        self.api_usage_count = 0
        self.last_processing_date: Optional[datetime] = None

        # é‡è¦åº¦ã—ãã„å€¤ï¼ˆç’°å¢ƒå¤‰æ•°åˆ¶å¾¡ï¼‰
        self.min_importance_score = float(os.getenv('MIN_IMPORTANCE_SCORE', '0.4'))

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆPhase 2.3çµ±åˆï¼‰
        self.performance_metrics = {
            'daily_consolidation_operations': 0,
            'unified_analysis_operations': 0,
            'batch_embedding_operations': 0,
            'progress_differential_operations': 0,
            'memory_storage_operations': 0,
            'total_operations': 0
        }

    async def initialize(self):
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        await self.memory_system.initialize()
        self.logger.info("Long-term Memory Processor initialized")

    async def _measure_performance(self, operation_name: str, func, *args, **kwargs):
        """å„æ“ä½œã®è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¸¬å®šï¼ˆPhase 2.3çµ±åˆï¼‰"""
        start_time = asyncio.get_event_loop().time()
        memory_before = psutil.Process().memory_info().rss if psutil else 0

        try:
            result = await func(*args, **kwargs)
            success = True
        finally:
            elapsed_time = asyncio.get_event_loop().time() - start_time
            memory_after = psutil.Process().memory_info().rss if psutil else 0
            memory_delta = memory_after - memory_before

            self.logger.info(f"Performance: {operation_name}", extra={
                'elapsed_ms': int(elapsed_time * 1000),
                'memory_delta_mb': memory_delta / 1024 / 1024 if psutil else 0,
                'success': success
            })

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.performance_metrics['total_operations'] += 1

        return result

    async def daily_memory_consolidation(
            self, target_date: Optional[datetime] = None
    ) -> Tuple[List[ProcessedMemory], ProgressDifferential]:
        """
        æ—¥æ¬¡è¨˜æ†¶çµ±åˆå‡¦ç†ï¼ˆ3-APIï¼‰

        Returns:
            (å‡¦ç†æ¸ˆã¿è¨˜æ†¶ä¸€è¦§, é€²æ—å·®åˆ†)
        """
        # Phase 3: ç’°å¢ƒå¤‰æ•°åˆ¶å¾¡ãƒã‚§ãƒƒã‚¯
        if not self.is_enabled:
            self.logger.info("ğŸš« Long-term memory processing is disabled (LONG_TERM_MEMORY_ENABLED=false)")
            return [], ProgressDifferential(
                date=target_date or datetime.now(),
                new_entities=[],
                progressed_entities=[],
                stagnant_entities=[],
                completed_tasks=[],
                new_skills=[],
                overall_summary="é•·æœŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ ãŒç„¡åŠ¹ã§ã™"
            )

        if target_date is None:
            target_date = datetime.now()

        # APIä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
        if not self._check_api_quota(target_date):
            raise RuntimeError(f"APIä½¿ç”¨é‡åˆ¶é™è¶…éï¼ˆ1æ—¥{self.daily_api_limit}å›åˆ¶é™ï¼‰")

        self.logger.info(f"ğŸ§  é•·æœŸè¨˜æ†¶åŒ–å‡¦ç†é–‹å§‹: {target_date.strftime('%Y-%m-%d')}")
        start_time = datetime.now()

        try:
            # 1. Redisã‹ã‚‰ã®è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰
            raw_memories = await self._measure_performance(
                "daily_memory_fetch",
                self._fetch_daily_memories,
                target_date
            )
            self.logger.info(f"ğŸ“Š å–å¾—è¨˜æ†¶æ•°: {len(raw_memories)}ä»¶")

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.performance_metrics['daily_consolidation_operations'] += 1

            if not raw_memories:
                self.logger.error("å‡¦ç†å¯¾è±¡è¨˜æ†¶ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                raise ValueError(f"No processable memories found for date {target_date}")

            # 2. API 1: Gemini 2.0 Flashçµ±åˆåˆ†æï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰
            processed_memories = await self._measure_performance(
                "unified_analysis",
                self._api1_unified_analysis,
                raw_memories
            )
            self.api_usage_count += 1
            self.performance_metrics['unified_analysis_operations'] += 1
            self.logger.info(f"âœ… API 1å®Œäº†: {len(processed_memories)}ä»¶å‡¦ç†")

            # 3. MinHashé‡è¤‡é™¤å»ï¼ˆAPIä¸è¦ï¼‰
            unique_memories = self._remove_duplicates(processed_memories)
            duplicate_ratio = (len(processed_memories) - len(unique_memories)) / len(processed_memories) * 100
            self.logger.info(f"ğŸ” é‡è¤‡é™¤å»: {duplicate_ratio:.1f}% ({len(unique_memories)}ä»¶æ®‹å­˜)")

            # 4. API 2: é€²æ—å·®åˆ†ç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰
            progress_diff = await self._measure_performance(
                "progress_differential",
                self._api2_progress_differential,
                unique_memories, target_date
            )
            self.api_usage_count += 1
            self.performance_metrics['progress_differential_operations'] += 1
            self.logger.info("âœ… API 2å®Œäº†: é€²æ—å·®åˆ†ç”Ÿæˆ")

            # 5. API 3: ãƒãƒƒãƒembeddingç”Ÿæˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰
            final_memories = await self._measure_performance(
                "batch_embeddings",
                self._api3_batch_embeddings,
                unique_memories
            )
            self.api_usage_count += 1
            self.performance_metrics['batch_embedding_operations'] += 1
            self.logger.info(f"âœ… API 3å®Œäº†: {len(final_memories)}ä»¶embeddingç”Ÿæˆ")

            # 6. PostgreSQLä¿å­˜ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ä»˜ãï¼‰
            saved_count = await self._measure_performance(
                "unified_memory_storage",
                self._store_unified_memories,
                final_memories
            )
            self.performance_metrics['memory_storage_operations'] += 1
            self.logger.info(f"ğŸ’¾ PostgreSQLä¿å­˜: {saved_count}ä»¶")

            processing_time = (datetime.now() - start_time).total_seconds()
            self.logger.info(
                f"ğŸ‰ é•·æœŸè¨˜æ†¶åŒ–å®Œäº†: {processing_time:.1f}ç§’, "
                f"APIä½¿ç”¨é‡: {self.daily_api_limit}å›",
                extra={
                    "processing_time_seconds": processing_time,
                    "total_memories_processed": len(final_memories),
                    "performance_metrics": self.performance_metrics
                }
            )

            return final_memories, progress_diff

        except (redis.RedisError, asyncpg.PostgresError) as e:
            self.logger.error(f"âŒ é•·æœŸè¨˜æ†¶åŒ–æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            raise MemorySystemConnectionError(
                f"Long-term memory processing failed: {e}"
            )
        except Exception as e:
            self.logger.error(f"âŒ é•·æœŸè¨˜æ†¶åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            raise MemorySystemError(f"Long-term memory processing failed: {e}")

    async def _fetch_daily_memories(self, target_date: datetime) -> List[Dict[str, Any]]:
        """Redis ã‹ã‚‰æŒ‡å®šæ—¥ã®è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        date_key = target_date.strftime('%Y-%m-%d')

        # Redisæ¥ç¶š
        redis_url = self.memory_system.redis_url
        if not redis_url:
            raise ValueError("Redis URL not available")
        redis_client = redis.from_url(redis_url)

        # æ—¥æ¬¡è¨˜æ†¶ã‚­ãƒ¼å–å¾—ï¼ˆPhase 3çµ±ä¸€ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
        pattern = f"daily_memory:{date_key}:*"
        keys = await redis_client.keys(pattern)

        memories = []
        for key in keys:
            data = await redis_client.hgetall(key)
            if data:
                # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
                decoded_data = {k.decode(): v.decode() for k, v in data.items()}
                memories.append(decoded_data)

        await redis_client.close()
        return memories

    async def _api1_unified_analysis(self, raw_memories: List[Dict[str, Any]]) -> List[ProcessedMemory]:
        """API 1: Gemini 2.0 Flashçµ±åˆåˆ†æ"""
        self.logger.info("ğŸ” API 1: Gemini 2.0 Flashçµ±åˆåˆ†æé–‹å§‹")

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        analysis_prompt = self._build_unified_analysis_prompt(raw_memories)

        # Gemini 2.0 Flashå®Ÿè¡Œ
        response = await self.gemini_flash.ainvoke(analysis_prompt)

        # å¿œç­”ãƒ‘ãƒ¼ã‚¹
        processed_memories = self._parse_analysis_response(response, raw_memories)

        return processed_memories

    def _build_unified_analysis_prompt(self, raw_memories: List[Dict[str, Any]]) -> str:
        """çµ±åˆåˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        memories_json = json.dumps(raw_memories, ensure_ascii=False, indent=2)

        return f"""
# Discordè¨˜æ†¶çµ±åˆåˆ†æã‚¿ã‚¹ã‚¯

## å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
ä»¥ä¸‹ã¯1æ—¥åˆ†ã®Discordä¼šè©±è¨˜æ†¶ãƒ‡ãƒ¼ã‚¿ã§ã™ï¼š

```json
{memories_json}
```

## åˆ†æè¦æ±‚

å„è¨˜æ†¶ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

### 1. ä¾¡å€¤åˆ¤å®šï¼ˆ0.0-1.0ï¼‰
- 0.8-1.0: é‡è¦ï¼ˆæŠ€è¡“ç¿’å¾—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²å±•ã€é‡è¦æ±ºå®šï¼‰
- 0.5-0.7: ä¸­ç¨‹åº¦ï¼ˆæ—¥å¸¸ä½œæ¥­ã€è­°è«–ã€ã‚¢ã‚¤ãƒ‡ã‚¢ï¼‰
- 0.0-0.4: ä½ï¼ˆé›‘è«‡ã€é‡è¤‡æƒ…å ±ï¼‰

### 2. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡º
æŠ€è¡“ã€äººç‰©ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ã‚¹ã‚­ãƒ«ã€ãƒ„ãƒ¼ãƒ«ç­‰ã‚’æŠ½å‡º
ä¾‹: {{"name": "TypeScript", "type": "technology"}}

### 3. è¨˜æ†¶ã‚¿ã‚¤ãƒ—åˆ†é¡
- conversation: é€šå¸¸ä¼šè©±
- task: ã‚¿ã‚¹ã‚¯é–¢é€£
- progress: é€²æ—å ±å‘Š
- learning: å­¦ç¿’è¨˜éŒ²
- decision: æ„æ€æ±ºå®š

### 4. æ§‹é€ åŒ–
å†…å®¹ã‚’ç°¡æ½”ã§æ¤œç´¢ã—ã‚„ã™ã„å½¢å¼ã«æ§‹é€ åŒ–

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
```json
[
  {{
    "id": "è¨˜æ†¶ID",
    "structured_content": "æ§‹é€ åŒ–ã•ã‚ŒãŸå†…å®¹",
    "memory_type": "åˆ†é¡",
    "entities": [
      {{"name": "ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£å", "type": "ã‚¿ã‚¤ãƒ—"}}
    ],
    "importance_score": 0.8,
    "progress_indicators": {{
      "skill_development": "ç¿’å¾—æŠ€è¡“",
      "project_advancement": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²å±•",
      "problem_resolution": "è§£æ±ºèª²é¡Œ"
    }}
  }}
]
```

é‡è¦æ€§0.4æœªæº€ã¯é™¤å¤–ã—ã¦ãã ã•ã„ã€‚ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºã¯åŒ…æ‹¬çš„ã«è¡Œã„ã€å€‹åˆ¥å…·ä½“çš„è¦ç´ ï¼ˆTypeScriptã€è»¢è·ç­‰ï¼‰ãŒæ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
"""

    def _parse_analysis_response(self, response: str, raw_memories: List[Dict[str, Any]]) -> List[ProcessedMemory]:
        """åˆ†æå¿œç­”ãƒ‘ãƒ¼ã‚¹"""
        try:
            # JSONéƒ¨åˆ†æŠ½å‡º
            json_start = response.find('[')
            json_end = response.rfind(']') + 1
            json_content = response[json_start:json_end]

            analysis_results = json.loads(json_content)

            # ProcessedMemoryä½œæˆ
            processed_memories = []
            for result in analysis_results:
                # å…ƒè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿æ¤œç´¢
                original_memory = next(
                    (m for m in raw_memories if m.get('id') == result['id']),
                    None
                )

                if original_memory:
                    processed_memory = ProcessedMemory(
                        id=result['id'],
                        original_content=original_memory.get('content', ''),
                        structured_content=result['structured_content'],
                        timestamp=datetime.fromisoformat(original_memory.get('timestamp', datetime.now().isoformat())),
                        channel_id=int(original_memory.get('channel_id', 0)),
                        user_id=original_memory.get('user_id', ''),
                        memory_type=result['memory_type'],
                        entities=result['entities'],
                        importance_score=result['importance_score'],
                        progress_indicators=result.get('progress_indicators', {}),
                        metadata=original_memory.get('metadata', {})
                    )
                    processed_memories.append(processed_memory)

            return processed_memories

        except Exception as e:
            self.logger.error(f"åˆ†æå¿œç­”ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            raise MemorySystemError(f"Analysis response parsing failed: {e}")

    def _remove_duplicates(self, processed_memories: List[ProcessedMemory]) -> List[ProcessedMemory]:
        """MinHashé‡è¤‡é™¤å»"""
        memory_items = []
        for memory in processed_memories:
            item = MemoryItem(
                id=memory.id,
                content=memory.structured_content,
                timestamp=memory.timestamp,
                channel_id=memory.channel_id,
                user_id=memory.user_id,
                memory_type=memory.memory_type,
                metadata=memory.metadata
            )
            memory_items.append(item)

        # é‡è¤‡é™¤å»å®Ÿè¡Œ
        unique_items = self.deduplicator.batch_deduplicate(memory_items)

        # ProcessedMemoryå½¢å¼ã«æˆ»ã™
        unique_ids = {item.id for item in unique_items}
        unique_memories = [m for m in processed_memories if m.id in unique_ids]

        # MinHashã‚·ã‚°ãƒãƒãƒ£è¿½åŠ 
        signatures = self.deduplicator.export_minhash_signatures()
        for memory in unique_memories:
            memory.minhash_signature = signatures.get(memory.id)

        return unique_memories

    async def _api2_progress_differential(
            self,
            memories: List[ProcessedMemory],
            target_date: datetime) -> ProgressDifferential:
        """API 2: é€²æ—å·®åˆ†ç”Ÿæˆ"""
        self.logger.info("ğŸ“ˆ API 2: é€²æ—å·®åˆ†ç”Ÿæˆé–‹å§‹")

        # å‰æ—¥è¨˜æ†¶å–å¾—
        previous_date = target_date - timedelta(days=1)
        previous_snapshot = await self._get_previous_memory_snapshot(previous_date)

        # å·®åˆ†åˆ†æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        diff_prompt = self._build_progress_diff_prompt(memories, previous_snapshot, target_date)

        # Geminiå®Ÿè¡Œ
        response = await self.gemini_flash.ainvoke(diff_prompt)

        # å·®åˆ†çµæœãƒ‘ãƒ¼ã‚¹
        progress_diff = self._parse_progress_diff_response(response, target_date)

        return progress_diff

    async def _api3_batch_embeddings(self, memories: List[ProcessedMemory]) -> List[ProcessedMemory]:
        """API 3: ãƒãƒƒãƒembeddingç”Ÿæˆï¼ˆPhase 1çµ±åˆï¼‰"""
        self.logger.info("ğŸ”— API 3: ãƒãƒƒãƒembeddingç”Ÿæˆé–‹å§‹")

        # å…¨ãƒ†ã‚­ã‚¹ãƒˆçµåˆ
        texts = [memory.structured_content for memory in memories]

        # GoogleEmbeddingClientãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
        embeddings = await self.embeddings_client.embed_documents_batch(texts)

        # embeddingå‰²ã‚Šå½“ã¦
        for memory, embedding in zip(memories, embeddings):
            memory.embedding = embedding

        return memories

    async def _store_unified_memories(self, memories: List[ProcessedMemory]) -> int:
        """PostgreSQLçµ±ä¸€è¨˜æ†¶ä¿å­˜"""
        if not memories:
            return 0

        # PostgreSQLæ¥ç¶š
        conn = await asyncpg.connect(self.memory_system.postgres_url)

        try:
            saved_count = 0
            for memory in memories:
                await conn.execute("""
                    INSERT INTO unified_memories (
                        id, memory_timestamp, channel_id, user_id, message_id,
                        content, memory_type, metadata, embedding,
                        minhash_signature, entities, progress_type, importance_score
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                    ON CONFLICT (id) DO NOTHING
                """,
                    memory.id,
                    memory.timestamp,
                    memory.channel_id,
                    memory.user_id,
                    None,  # message_id
                    memory.structured_content,
                    memory.memory_type,
                    json.dumps(memory.metadata),
                    memory.embedding,
                    memory.minhash_signature,
                    json.dumps(memory.entities),
                    memory.progress_indicators.get('project_advancement', ''),
                    memory.importance_score
                )
                saved_count += 1

            return saved_count

        finally:
            await conn.close()

    def _check_api_quota(self, target_date: datetime) -> bool:
        """APIä½¿ç”¨é‡åˆ¶é™ãƒã‚§ãƒƒã‚¯ï¼ˆ1æ—¥3å›ï¼‰"""
        current_date = target_date.date()
        
        # last_processing_dateãŒdatetimeå‹ã®å ´åˆã¯date()ã§å¤‰æ›
        last_date = self.last_processing_date.date() if self.last_processing_date else None

        if last_date != current_date:
            # æ–°ã—ã„æ—¥ - ãƒªã‚»ãƒƒãƒˆ
            self.api_usage_count = 0
            self.last_processing_date = target_date
            return True

        return self.api_usage_count < self.daily_api_limit

    # ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆprogress_diffé–¢é€£ï¼‰ã¯ç°¡ç•¥åŒ–ã—ã¦å¾Œã§å®Ÿè£…
    async def _get_previous_memory_snapshot(self, date: datetime) -> Dict[str, Any]:
        """å‰æ—¥è¨˜æ†¶ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        return {"entities": [], "summary": "å‰æ—¥ãƒ‡ãƒ¼ã‚¿ãªã—"}

    def _build_progress_diff_prompt(self, memories, previous_snapshot, target_date) -> str:
        """é€²æ—å·®åˆ†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        return f"é€²æ—å·®åˆ†ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚æ—¥ä»˜: {target_date.strftime('%Y-%m-%d')}"

    def _parse_progress_diff_response(self, response: str, target_date: datetime) -> ProgressDifferential:
        """é€²æ—å·®åˆ†å¿œç­”ãƒ‘ãƒ¼ã‚¹ï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        return ProgressDifferential(
            date=target_date,
            new_entities=[],
            progressed_entities=[],
            stagnant_entities=[],
            completed_tasks=[],
            new_skills=[],
            overall_summary="é€²æ—å·®åˆ†å‡¦ç†å®Œäº†"
        )

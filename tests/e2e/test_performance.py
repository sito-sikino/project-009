"""
Performance Tests - TDD Phase 7 (E2E Performance Validation)

ãƒ†ã‚¹ãƒˆå¯¾è±¡: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§
ç›®çš„: 9ç§’å¿œç­”æ™‚é–“ãƒ»95%ç²¾åº¦ãƒ»APIåŠ¹ç‡è¦ä»¶æ¤œè¨¼
"""

import pytest
import pytest_asyncio
import asyncio
import time
import statistics
from unittest.mock import AsyncMock, MagicMock, patch
from typing import Dict, Any, List
# import numpy as np  # ä»£ã‚ã‚Šã«statisticsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ 
try:
    from src.bots.reception import ReceptionClient
    from src.core.message_processor import PriorityQueue
    from src.agents.supervisor import AgentSupervisor
    from src.infrastructure.gemini_client import GeminiClient
    from src.bots.output_bots import SpectraBot, LynQBot, PazBot
    from src.infrastructure.message_router import MessageRouter
except ImportError as e:
    pytest.skip(f"Performance test dependencies not available: {e}", allow_module_level=True)


class TestPerformanceBenchmarks:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼ãƒ†ã‚¹ãƒˆ"""
    
    @pytest_asyncio.fixture
    async def performance_system(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # PriorityQueue
        priority_queue = PriorityQueue()
        
        # Reception Client
        reception_client = ReceptionClient(priority_queue=priority_queue)
        reception_client._test_user_id = 555666777
        
        # Gemini Client (å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã‚’ãƒ¢ãƒƒã‚¯)
        gemini_client = GeminiClient(api_key="test_performance_key")
        
        # Agent Supervisor
        supervisor = AgentSupervisor(
            gemini_client=gemini_client,
            memory_system=None
        )
        
        # Output Bots
        spectra_bot = SpectraBot(token="test_spectra_token")
        lynq_bot = LynQBot(token="test_lynq_token")
        paz_bot = PazBot(token="test_paz_token")
        
        # Mock Discord operations for performance testing
        for bot in [spectra_bot, lynq_bot, paz_bot]:
            bot.get_channel = MagicMock()
            bot.get_channel.return_value = MagicMock()
            bot.get_channel.return_value.send = AsyncMock()
        
        # Message Router
        bots = {
            "spectra": spectra_bot,
            "lynq": lynq_bot,
            "paz": paz_bot
        }
        router = MessageRouter(bots=bots)
        
        return {
            "reception_client": reception_client,
            "priority_queue": priority_queue,
            "supervisor": supervisor,
            "gemini_client": gemini_client,
            "router": router,
            "bots": bots
        }
    
    def create_test_message(self, content: str, is_mention: bool = False) -> MagicMock:
        """ãƒ†ã‚¹ãƒˆç”¨Discordãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ"""
        message = MagicMock()
        message.id = 123456789
        message.content = content
        message.author = MagicMock()
        message.author.id = 987654321
        message.author.bot = False
        message.channel = MagicMock()
        message.channel.id = 111222333
        message.guild = MagicMock()
        message.guild.id = 444555666
        
        if is_mention:
            bot_user = MagicMock()
            bot_user.id = 555666777
            message.mentions = [bot_user]
        else:
            message.mentions = []
        
        return message

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_response_time_95_percent_under_9_seconds(self, performance_system):
        """å¿œç­”æ™‚é–“ï¼š95%ãŒ9ç§’ä»¥å†…å®Œäº†ãƒ†ã‚¹ãƒˆ"""
        
        # ARRANGE: ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        reception = performance_system["reception_client"]
        priority_queue = performance_system["priority_queue"]
        supervisor = performance_system["supervisor"]
        router = performance_system["router"]
        
        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ã‚¿ãƒ¼ãƒ³
        test_messages = [
            "è­°è«–ã‚’æ•´ç†ã—ã¦ãã ã•ã„",
            "æŠ€è¡“çš„ãªå•é¡Œã‚’åˆ†æã—ã¦",
            "æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°",
            "ä¼šè­°ã®é€²è¡Œã‚’ãŠé¡˜ã„ã—ã¾ã™",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã«ã¤ã„ã¦ç›¸è«‡",
            "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªè§£æ±ºç­–ã‚’è€ƒãˆã¦",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…¨ä½“åƒã‚’æ•´ç†",
            "è«–ç†çš„ã«æ¤œè¨¼ã—ã¦ãã ã•ã„",
            "é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆ",
            "è­°äº‹éŒ²ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„"
        ]
        
        response_times = []
        
        # Gemini API ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒƒã‚¯ï¼ˆé«˜é€ŸåŒ–ï¼‰
        mock_responses = {
            "è­°è«–": {"selected_agent": "spectra", "response_content": "è­°è«–ã‚’æ•´ç†ã—ã¾ã™", "confidence": 0.95, "reasoning": "ãƒ¡ã‚¿é€²è¡Œ"},
            "æŠ€è¡“": {"selected_agent": "lynq", "response_content": "æŠ€è¡“åˆ†æã—ã¾ã™", "confidence": 0.90, "reasoning": "è«–ç†åˆ†æ"},
            "ã‚¢ã‚¤ãƒ‡ã‚¢": {"selected_agent": "paz", "response_content": "ã‚¢ã‚¤ãƒ‡ã‚¢ææ¡ˆã—ã¾ã™", "confidence": 0.88, "reasoning": "å‰µé€ ç™ºæ•£"},
            "ä¼šè­°": {"selected_agent": "spectra", "response_content": "é€²è¡Œã—ã¾ã™", "confidence": 0.92, "reasoning": "ãƒ¡ã‚¿é€²è¡Œ"},
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹": {"selected_agent": "lynq", "response_content": "è¨­è¨ˆåˆ†æã—ã¾ã™", "confidence": 0.94, "reasoning": "æŠ€è¡“æ¤œè¨¼"},
            "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–": {"selected_agent": "paz", "response_content": "å‰µé€ çš„è§£æ±ºç­–ã§ã™", "confidence": 0.89, "reasoning": "å‰µé€ ç™ºæ•£"},
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ": {"selected_agent": "spectra", "response_content": "å…¨ä½“æ•´ç†ã—ã¾ã™", "confidence": 0.93, "reasoning": "ãƒ¡ã‚¿é€²è¡Œ"},
            "è«–ç†": {"selected_agent": "lynq", "response_content": "è«–ç†æ¤œè¨¼ã—ã¾ã™", "confidence": 0.91, "reasoning": "è«–ç†åæŸ"},
            "é©æ–°": {"selected_agent": "paz", "response_content": "é©æ–°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™", "confidence": 0.87, "reasoning": "å‰µé€ ç™ºæ•£"},
            "è­°äº‹éŒ²": {"selected_agent": "spectra", "response_content": "ã¾ã¨ã‚ã¾ã™", "confidence": 0.96, "reasoning": "ãƒ¡ã‚¿é€²è¡Œ"}
        }
        
        with patch.object(supervisor.gemini_client, '_call_gemini_api', new_callable=AsyncMock) as mock_api:
            
            # ACT: 20å›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆè»½é‡åŒ–ï¼‰
            for i in range(20):
                message_content = test_messages[i % len(test_messages)]
                
                # APIå¿œç­”ãƒ¢ãƒƒã‚¯è¨­å®š
                key = next((k for k in mock_responses.keys() if k in message_content), "è­°è«–")
                mock_api.return_value = mock_responses[key]
                
                test_message = self.create_test_message(message_content)
                
                # å®Œå…¨ãƒ•ãƒ­ãƒ¼æ™‚é–“æ¸¬å®š
                start_time = time.time()
                
                # Step 1-4: å®Œå…¨ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
                await reception.on_message(test_message)
                message_data = await priority_queue.dequeue()
                
                initial_state = {
                    'messages': [{'role': 'user', 'content': message_data['message'].content}],
                    'channel_id': str(message_data['message'].channel.id)
                }
                supervisor_result = await supervisor.process_message(initial_state)
                await router.route_message(supervisor_result)
                
                elapsed_time = time.time() - start_time
                response_times.append(elapsed_time)
                
                # é€²æ—è¡¨ç¤ºï¼ˆ5å›ã”ã¨ï¼‰
                if (i + 1) % 5 == 0:
                    print(f"Progress: {i + 1}/20 tests completed")
        
        # ASSERT: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶æ¤œè¨¼
        # 95th percentileã®è¨ˆç®— (numpyã®ä»£æ›¿)
        sorted_times = sorted(response_times)
        percentile_95_index = int(len(sorted_times) * 0.95)
        percentile_95 = sorted_times[percentile_95_index]
        average_time = statistics.mean(response_times)
        max_time = max(response_times)
        min_time = min(response_times)
        
        print(f"\nğŸ“Š Performance Results:")
        print(f"95th percentile: {percentile_95:.3f}s")
        print(f"Average: {average_time:.3f}s")
        print(f"Max: {max_time:.3f}s")
        print(f"Min: {min_time:.3f}s")
        
        # 9ç§’ä»¥å†…è¦ä»¶æ¤œè¨¼
        assert percentile_95 < 9.0, f"95th percentile ({percentile_95:.3f}s) exceeds 9 second requirement"
        
        # è¿½åŠ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
        assert average_time < 5.0, f"Average response time ({average_time:.3f}s) should be under 5 seconds"
        assert len([t for t in response_times if t > 9.0]) < 5, "More than 5% of responses exceeded 9 seconds"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_agent_selection_accuracy_95_percent(self, performance_system):
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠç²¾åº¦ï¼š95%ä»¥ä¸Šãƒ†ã‚¹ãƒˆ"""
        
        supervisor = performance_system["supervisor"]
        
        # æ˜ç¢ºãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_scenarios = [
            # SpectraæœŸå¾…ã‚±ãƒ¼ã‚¹
            {"content": "è­°è«–ã‚’æ•´ç†ã—ã¦ãã ã•ã„", "expected": "spectra", "category": "meta"},
            {"content": "ä¼šè­°ã®é€²è¡Œã‚’ãŠé¡˜ã„ã—ã¾ã™", "expected": "spectra", "category": "meta"},
            {"content": "å…¨ä½“ã®æ–¹é‡ã‚’æ•´ç†ã—ãŸã„", "expected": "spectra", "category": "meta"},
            {"content": "è­°äº‹éŒ²ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„", "expected": "spectra", "category": "meta"},
            {"content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ åŒ–ã‚’", "expected": "spectra", "category": "meta"},
            
            # LynQæœŸå¾…ã‚±ãƒ¼ã‚¹
            {"content": "æŠ€è¡“çš„ãªå•é¡Œã‚’åˆ†æã—ã¦", "expected": "lynq", "category": "logical"},
            {"content": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã«ã¤ã„ã¦", "expected": "lynq", "category": "logical"},
            {"content": "è«–ç†çš„ã«æ¤œè¨¼ã—ã¦ãã ã•ã„", "expected": "lynq", "category": "logical"},
            {"content": "ã‚·ã‚¹ãƒ†ãƒ æ§‹é€ ã‚’åˆ†æ", "expected": "lynq", "category": "logical"},
            {"content": "å•é¡Œè§£æ±ºã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’", "expected": "lynq", "category": "logical"},
            
            # PazæœŸå¾…ã‚±ãƒ¼ã‚¹
            {"content": "æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°", "expected": "paz", "category": "creative"},
            {"content": "ã‚¯ãƒªã‚¨ã‚¤ãƒ†ã‚£ãƒ–ãªè§£æ±ºç­–ã‚’", "expected": "paz", "category": "creative"},
            {"content": "é©æ–°çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆ", "expected": "paz", "category": "creative"},
            {"content": "æ–¬æ–°ãªãƒ‡ã‚¶ã‚¤ãƒ³ã‚¢ã‚¤ãƒ‡ã‚¢", "expected": "paz", "category": "creative"},
            {"content": "å‰µé€ çš„ãªç™ºæƒ³ã§è€ƒãˆã¦", "expected": "paz", "category": "creative"}
        ]
        
        correct_selections = 0
        total_tests = len(test_scenarios) * 2  # å„ã‚·ãƒŠãƒªã‚ªã‚’2å›ãƒ†ã‚¹ãƒˆï¼ˆè»½é‡åŒ–ï¼‰
        
        with patch.object(supervisor.gemini_client, '_call_gemini_api', new_callable=AsyncMock) as mock_api:
            
            for scenario in test_scenarios:
                for iteration in range(2):  # å„ã‚±ãƒ¼ã‚¹2å›å®Ÿè¡Œã—ã¦å®‰å®šæ€§ç¢ºèª
                    
                    # ç¾å®Ÿçš„ãªAPIå¿œç­”ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                    if scenario["category"] == "meta":
                        mock_api.return_value = {
                            "selected_agent": "spectra",
                            "response_content": "ãƒ¡ã‚¿é€²è¡Œå½¹ã¨ã—ã¦å¯¾å¿œã—ã¾ã™",
                            "confidence": 0.90 + (iteration * 0.02),
                            "reasoning": "è­°è«–æ§‹é€ åŒ–ãƒ»é€²è¡Œç®¡ç†ãŒé©åˆ‡"
                        }
                    elif scenario["category"] == "logical":
                        mock_api.return_value = {
                            "selected_agent": "lynq", 
                            "response_content": "è«–ç†çš„ã«åˆ†æã—ã¾ã™",
                            "confidence": 0.88 + (iteration * 0.02),
                            "reasoning": "æŠ€è¡“çš„æ¤œè¨¼ãƒ»è«–ç†åˆ†æãŒå¿…è¦"
                        }
                    elif scenario["category"] == "creative":
                        mock_api.return_value = {
                            "selected_agent": "paz",
                            "response_content": "å‰µé€ çš„ã«ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ã¾ã™",
                            "confidence": 0.86 + (iteration * 0.02),
                            "reasoning": "å‰µé€ çš„ç™ºæƒ³ãƒ»é©æ–°çš„ã‚¢ã‚¤ãƒ‡ã‚¢ãŒé©åˆ‡"
                        }
                    
                    # ACT: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠå®Ÿè¡Œ
                    initial_state = {
                        'messages': [{'role': 'user', 'content': scenario["content"]}],
                        'channel_id': '12345'
                    }
                    
                    result = await supervisor.process_message(initial_state)
                    
                    # æ­£è§£åˆ¤å®š
                    if result['selected_agent'] == scenario["expected"]:
                        correct_selections += 1
        
        # ASSERT: 95%ç²¾åº¦è¦ä»¶æ¤œè¨¼
        accuracy = correct_selections / total_tests
        accuracy_percent = accuracy * 100
        
        print(f"\nğŸ¯ Agent Selection Accuracy Results:")
        print(f"Correct: {correct_selections}/{total_tests}")
        print(f"Accuracy: {accuracy_percent:.1f}%")
        
        assert accuracy >= 0.95, f"Agent selection accuracy ({accuracy_percent:.1f}%) below 95% requirement"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_api_efficiency_measurement(self, performance_system):
        """APIåŠ¹ç‡æ¸¬å®šï¼šçµ±åˆå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–æ¤œè¨¼"""
        
        supervisor = performance_system["supervisor"]
        
        # APIå‘¼ã³å‡ºã—å›æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        api_call_count = 0
        
        def count_api_calls(*args, **kwargs):
            nonlocal api_call_count
            api_call_count += 1
            return {
                "selected_agent": "spectra",
                "response_content": "çµ±åˆAPIå¿œç­”",
                "confidence": 0.90,
                "reasoning": "çµ±åˆå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–"
            }
        
        with patch.object(supervisor.gemini_client, '_call_gemini_api', new_callable=AsyncMock) as mock_api:
            mock_api.side_effect = count_api_calls
            
            # ACT: 50å›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
            for i in range(50):
                initial_state = {
                    'messages': [{'role': 'user', 'content': f'ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i}'}],
                    'channel_id': '12345'
                }
                
                await supervisor.process_message(initial_state)
            
            # ASSERT: APIåŠ¹ç‡æ€§æ¤œè¨¼
            # çµ±åˆå‡¦ç†ï¼š1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ = 1APIå‘¼ã³å‡ºã—
            # å¾“æ¥æ–¹å¼ï¼š1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ = 2APIå‘¼ã³å‡ºã—ï¼ˆé¸æŠ+ç”Ÿæˆï¼‰
            expected_traditional_calls = 50 * 2  # 100å›
            actual_calls = api_call_count  # 50å›
            
            efficiency_improvement = (expected_traditional_calls - actual_calls) / expected_traditional_calls
            efficiency_percent = efficiency_improvement * 100
            
            print(f"\nâš¡ API Efficiency Results:")
            print(f"Traditional approach: {expected_traditional_calls} API calls")
            print(f"Unified approach: {actual_calls} API calls")
            print(f"Efficiency improvement: {efficiency_percent:.1f}%")
            
            assert efficiency_improvement >= 0.50, f"API efficiency improvement ({efficiency_percent:.1f}%) below 50% target"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_concurrent_load_handling(self, performance_system):
        """ä¸¦è¡Œè² è·å‡¦ç†ï¼šåŒæ™‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†èƒ½åŠ›ãƒ†ã‚¹ãƒˆ"""
        
        reception = performance_system["reception_client"]
        priority_queue = performance_system["priority_queue"]
        supervisor = performance_system["supervisor"]
        router = performance_system["router"]
        
        # ä¸¦è¡Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        concurrent_messages = 20
        
        # ãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
        test_messages = [
            self.create_test_message(f"ä¸¦è¡Œãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ {i}")
            for i in range(concurrent_messages)
        ]
        
        with patch.object(supervisor.gemini_client, '_call_gemini_api', new_callable=AsyncMock) as mock_api:
            mock_api.return_value = {
                "selected_agent": "spectra",
                "response_content": "ä¸¦è¡Œå‡¦ç†å¿œç­”",
                "confidence": 0.90,
                "reasoning": "ä¸¦è¡Œè² è·ãƒ†ã‚¹ãƒˆ"
            }
            
            # ACT: ä¸¦è¡Œãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†
            start_time = time.time()
            
            # å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸¦è¡Œå—ä¿¡
            await asyncio.gather(*[
                reception.on_message(msg) for msg in test_messages
            ])
            
            # å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸¦è¡Œå‡¦ç†
            message_data_list = []
            for _ in range(concurrent_messages):
                message_data = await priority_queue.dequeue()
                message_data_list.append(message_data)
            
            # LangGraphå‡¦ç†ã‚’ä¸¦è¡Œå®Ÿè¡Œ
            supervisor_tasks = []
            for message_data in message_data_list:
                initial_state = {
                    'messages': [{'role': 'user', 'content': message_data['message'].content}],
                    'channel_id': str(message_data['message'].channel.id)
                }
                supervisor_tasks.append(supervisor.process_message(initial_state))
            
            supervisor_results = await asyncio.gather(*supervisor_tasks)
            
            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’ä¸¦è¡Œå®Ÿè¡Œ
            await asyncio.gather(*[
                router.route_message(result) for result in supervisor_results
            ])
            
            total_time = time.time() - start_time
            
            # ASSERT: ä¸¦è¡Œå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
            average_time_per_message = total_time / concurrent_messages
            
            print(f"\nğŸ”„ Concurrent Load Results:")
            print(f"Messages processed: {concurrent_messages}")
            print(f"Total time: {total_time:.3f}s")
            print(f"Average per message: {average_time_per_message:.3f}s")
            
            # ä¸¦è¡Œå‡¦ç†åŠ¹ç‡è¦ä»¶
            assert total_time < 15.0, f"Concurrent processing ({total_time:.3f}s) should complete within 15 seconds"
            assert average_time_per_message < 2.0, f"Average per message ({average_time_per_message:.3f}s) should be under 2 seconds"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_memory_access_performance_simulation(self):
        """ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
        
        # Hot Memory (Redis) ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        hot_memory_times = []
        for _ in range(100):
            start_time = time.time()
            # Redisæ“ä½œã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆéå¸¸ã«é«˜é€Ÿï¼‰
            await asyncio.sleep(0.001)  # 1ms
            elapsed_time = time.time() - start_time
            hot_memory_times.append(elapsed_time)
        
        # Cold Memory (PostgreSQL) ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        cold_memory_times = []
        for _ in range(20):
            start_time = time.time()
            # PostgreSQLæ¤œç´¢ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            await asyncio.sleep(0.050)  # 50ms
            elapsed_time = time.time() - start_time
            cold_memory_times.append(elapsed_time)
        
        # ASSERT: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹æ€§èƒ½è¦ä»¶
        avg_hot = statistics.mean(hot_memory_times)
        avg_cold = statistics.mean(cold_memory_times)
        
        print(f"\nğŸ§  Memory Performance Simulation:")
        print(f"Hot Memory (Redis) avg: {avg_hot:.3f}s")
        print(f"Cold Memory (PostgreSQL) avg: {avg_cold:.3f}s")
        
        assert avg_hot < 0.1, f"Hot memory access ({avg_hot:.3f}s) should be under 0.1s"
        assert avg_cold < 3.0, f"Cold memory access ({avg_cold:.3f}s) should be under 3.0s"


# TDD Performance Phaseç¢ºèªç”¨
if __name__ == "__main__":
    print("ğŸ”´ TDD Performance Phase: System Performance Validation")
    print("ã“ã‚Œã‚‰ã®ãƒ†ã‚¹ãƒˆã¯ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
    try:
        from src.agents.supervisor import AgentSupervisor
        from src.bots.output_bots import SpectraBot
        print("âœ… Performance tests ready: All components available")
    except ImportError as e:
        print(f"âŒ Performance test requirements not met: {e}")
#!/usr/bin/env python3
"""
v0.3.0 Long-term Memory System Integration Tests
é•·æœŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ
"""

import pytest
import asyncio
import os
import uuid
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch
from typing import List, Dict, Any

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.infrastructure.long_term_memory import LongTermMemoryProcessor, ProcessedMemory
from src.infrastructure.deduplication_system import MinHashDeduplicator, MemoryItem
from src.core.daily_report_system import (
    DailyReportGenerator, 
    IntegratedMessageSystem, 
    EventDrivenWorkflowOrchestrator
)


class TestLongTermMemoryProcessor:
    """é•·æœŸè¨˜æ†¶å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def processor(self):
        """LongTermMemoryProcessorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return LongTermMemoryProcessor(
            redis_url="redis://localhost:6379",
            postgres_url="postgresql://test:test@localhost:5432/test_db",
            gemini_api_key="test_api_key"
        )
    
    @pytest.fixture
    def sample_redis_memories(self):
        """ã‚µãƒ³ãƒ—ãƒ«Redisè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                "id": str(uuid.uuid4()),
                "content": "TypeScriptã§Reactã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã—ã¦ã„ã¾ã™",
                "timestamp": datetime.now().isoformat(),
                "channel_id": "123456789",
                "user_id": "user001",
                "metadata": {"type": "development"}
            },
            {
                "id": str(uuid.uuid4()),
                "content": "è»¢è·æ´»å‹•ã®é€²æ—ã‚’å ±å‘Šã—ã¾ã™",
                "timestamp": datetime.now().isoformat(),
                "channel_id": "123456789",
                "user_id": "user002",
                "metadata": {"type": "personal"}
            },
            {
                "id": str(uuid.uuid4()),
                "content": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è€ƒãˆã¦ã„ã¾ã™",
                "timestamp": datetime.now().isoformat(),
                "channel_id": "987654321",
                "user_id": "user003",
                "metadata": {"type": "creation"}
            }
        ]
    
    @pytest.mark.asyncio
    async def test_api_quota_check(self, processor):
        """APIä½¿ç”¨é‡åˆ¶é™ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        target_date = datetime.now()
        
        # åˆå›ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ¶é™å†…ï¼‰
        assert processor._check_api_quota(target_date) == True
        
        # APIä½¿ç”¨é‡ã‚’3å›ã«è¨­å®š
        processor.api_usage_count = 3
        processor.last_processing_date = target_date.date()
        
        # åˆ¶é™è¶…éãƒã‚§ãƒƒã‚¯
        assert processor._check_api_quota(target_date) == False
        
        # æ–°ã—ã„æ—¥ã«ãªã£ãŸå ´åˆã®ãƒªã‚»ãƒƒãƒˆ
        next_day = target_date + timedelta(days=1)
        assert processor._check_api_quota(next_day) == True
        assert processor.api_usage_count == 0
    
    @pytest.mark.asyncio
    @patch('src.infrastructure.long_term_memory.redis.from_url')
    async def test_fetch_daily_memories(self, mock_redis, processor, sample_redis_memories):
        """Redisè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ†ã‚¹ãƒˆ"""
        # Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ¢ãƒƒã‚¯
        mock_redis_client = AsyncMock()
        mock_redis.return_value = mock_redis_client
        
        # Redisã‚­ãƒ¼æ¤œç´¢çµæœãƒ¢ãƒƒã‚¯
        mock_redis_client.keys.return_value = [
            b"memory:2024-01-15:msg001",
            b"memory:2024-01-15:msg002"
        ]
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœãƒ¢ãƒƒã‚¯
        mock_redis_client.hgetall.side_effect = [
            {b"id": b"test001", b"content": b"test content 1"},
            {b"id": b"test002", b"content": b"test content 2"}
        ]
        
        target_date = datetime(2024, 1, 15)
        memories = await processor._fetch_daily_memories(target_date)
        
        assert len(memories) == 2
        assert memories[0]["id"] == "test001"
        assert memories[1]["content"] == "test content 2"
        
        # Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¯ãƒ­ãƒ¼ã‚ºç¢ºèª
        mock_redis_client.close.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('src.infrastructure.long_term_memory.GoogleGenerativeAI')
    async def test_api1_unified_analysis(self, mock_gemini, processor, sample_redis_memories):
        """API 1: Geminiçµ±åˆåˆ†æãƒ†ã‚¹ãƒˆ"""
        # Geminiå¿œç­”ãƒ¢ãƒƒã‚¯
        mock_response = """
        [
          {
            "id": "test001",
            "structured_content": "TypeScript Reacté–‹ç™º",
            "memory_type": "learning",
            "entities": [{"name": "TypeScript", "type": "technology"}],
            "importance_score": 0.8,
            "progress_indicators": {"skill_development": "TypeScript"}
          }
        ]
        """
        
        mock_gemini_instance = AsyncMock()
        mock_gemini_instance.ainvoke.return_value = mock_response
        processor.gemini_flash = mock_gemini_instance
        
        processed = await processor._api1_unified_analysis(sample_redis_memories)
        
        assert len(processed) == 1
        assert processed[0].structured_content == "TypeScript Reacté–‹ç™º"
        assert processed[0].importance_score == 0.8
        assert len(processed[0].entities) == 1
        assert processed[0].entities[0]["name"] == "TypeScript"
        
        # Geminiå‘¼ã³å‡ºã—ç¢ºèª
        mock_gemini_instance.ainvoke.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('src.infrastructure.long_term_memory.GoogleGenerativeAIEmbeddings')
    async def test_api3_batch_embeddings(self, mock_embeddings, processor):
        """API 3: ãƒãƒƒãƒembeddingç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        # ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ¸ˆã¿è¨˜æ†¶
        memories = [
            ProcessedMemory(
                id="test001",
                original_content="test content",
                structured_content="structured test",
                timestamp=datetime.now(),
                channel_id=123,
                user_id="user001",
                memory_type="conversation",
                entities=[],
                importance_score=0.7,
                progress_indicators={},
                metadata={}
            )
        ]
        
        # Embeddingçµæœãƒ¢ãƒƒã‚¯
        mock_embeddings_instance = AsyncMock()
        mock_embeddings_instance.aembed_documents.return_value = [
            [0.1, 0.2, 0.3, 0.4]  # 4æ¬¡å…ƒãƒ†ã‚¹ãƒˆembedding
        ]
        processor.embeddings_client = mock_embeddings_instance
        
        result = await processor._api3_batch_embeddings(memories)
        
        assert len(result) == 1
        assert result[0].embedding == [0.1, 0.2, 0.3, 0.4]
        
        # Embedding APIå‘¼ã³å‡ºã—ç¢ºèª
        mock_embeddings_instance.aembed_documents.assert_called_once_with(["structured test"])


class TestMinHashDeduplicator:
    """é‡è¤‡æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def deduplicator(self):
        """MinHashDeduplicatorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return MinHashDeduplicator(threshold=0.8, num_perm=64)
    
    def test_content_normalization(self):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ"""
        from src.infrastructure.deduplication_system import ContentNormalizer
        
        # URLã€ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã€è¨˜å·ã®é™¤å»ãƒ†ã‚¹ãƒˆ
        original = "Hello @user123! Check https://example.com for more info!!!"
        normalized = ContentNormalizer.normalize_text(original)
        
        assert "https://example.com" not in normalized
        assert "@user123" not in normalized
        assert normalized.lower() == normalized  # å°æ–‡å­—åŒ–ç¢ºèª
    
    def test_memory_deduplication(self, deduplicator):
        """è¨˜æ†¶é‡è¤‡é™¤å»ãƒ†ã‚¹ãƒˆ"""
        # é¡ä¼¼ã—ãŸè¨˜æ†¶ã‚¢ã‚¤ãƒ†ãƒ 
        memory1 = MemoryItem(
            id="mem001",
            content="TypeScriptã§Reactã‚¢ãƒ—ãƒªã‚’é–‹ç™ºã—ã¦ã„ã¾ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation",
            metadata={}
        )
        
        memory2 = MemoryItem(
            id="mem002", 
            content="Reactã‚¢ãƒ—ãƒªã‚’TypeScriptã§é–‹ç™ºä¸­ã§ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation",
            metadata={}
        )
        
        memory3 = MemoryItem(
            id="mem003",
            content="Pythonã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…ã‚’ã—ã¦ã„ã¾ã™",
            timestamp=datetime.now(),
            channel_id=456,
            user_id="user002",
            memory_type="task",
            metadata={}
        )
        
        # ãƒãƒƒãƒé‡è¤‡é™¤å»å®Ÿè¡Œ
        unique_memories = deduplicator.batch_deduplicate([memory1, memory2, memory3])
        
        # é¡ä¼¼è¨˜æ†¶ï¼ˆmemory1, memory2ï¼‰ã¯1ã¤ã«é‡è¤‡é™¤å»ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
        assert len(unique_memories) <= 2
        assert any(mem.id == memory3.id for mem in unique_memories)  # memory3ã¯æ®‹å­˜
    
    def test_similarity_calculation(self, deduplicator):
        """é¡ä¼¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        memory1 = MemoryItem(
            id="mem001",
            content="åŒã˜å†…å®¹ã§ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation", 
            metadata={}
        )
        
        memory2 = MemoryItem(
            id="mem002",
            content="åŒã˜å†…å®¹ã§ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation",
            metadata={}
        )
        
        # è¨˜æ†¶è¿½åŠ 
        deduplicator.add_memory(memory1)
        deduplicator.add_memory(memory2)
        
        # é¡ä¼¼åº¦è¨ˆç®—
        similarity = deduplicator.get_similarity("mem001", "mem002")
        
        # å®Œå…¨ä¸€è‡´ãªã®ã§é«˜ã„é¡ä¼¼åº¦
        assert similarity > 0.8


class TestDailyReportSystem:
    """æ—¥å ±ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def report_generator(self):
        """DailyReportGeneratorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return DailyReportGenerator()
    
    @pytest.fixture
    def sample_processed_memories(self):
        """ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ¸ˆã¿è¨˜æ†¶"""
        return [
            ProcessedMemory(
                id="mem001",
                original_content="TypeScriptå­¦ç¿’ä¸­",
                structured_content="TypeScriptæŠ€è¡“ç¿’å¾—",
                timestamp=datetime.now(),
                channel_id=123,
                user_id="user001",
                memory_type="learning",
                entities=[{"name": "TypeScript", "type": "technology"}],
                importance_score=0.8,
                progress_indicators={"skill_development": "TypeScript"},
                metadata={}
            ),
            ProcessedMemory(
                id="mem002",
                original_content="æ–°ã—ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¢ã‚¤ãƒ‡ã‚¢",
                structured_content="UIãƒ‡ã‚¶ã‚¤ãƒ³ä¼ç”»",
                timestamp=datetime.now(),
                channel_id=456,
                user_id="user002",
                memory_type="progress",
                entities=[{"name": "UIãƒ‡ã‚¶ã‚¤ãƒ³", "type": "creative"}],
                importance_score=0.7,
                progress_indicators={"project_advancement": "ãƒ‡ã‚¶ã‚¤ãƒ³ä¼ç”»"},
                metadata={}
            )
        ]
    
    def test_daily_report_generation(self, report_generator, sample_processed_memories):
        """æ—¥å ±ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        from src.infrastructure.long_term_memory import ProgressDifferential
        
        progress_diff = ProgressDifferential(
            date=datetime.now(),
            new_entities=["TypeScript"],
            progressed_entities=["UIãƒ‡ã‚¶ã‚¤ãƒ³"],
            stagnant_entities=[],
            completed_tasks=[],
            new_skills=["TypeScript"],
            overall_summary="æŠ€è¡“å­¦ç¿’ã¨å‰µä½œæ´»å‹•ãŒé€²å±•"
        )
        
        processing_stats = {
            "processing_time": 8.5,
            "memory_count": 2,
            "api_usage": 3
        }
        
        daily_report = report_generator.generate_daily_report(
            sample_processed_memories, progress_diff, processing_stats
        )
        
        assert daily_report.date.date() == datetime.now().date()
        assert len(daily_report.departments) == 3  # Command Center, Creation, Development
        assert daily_report.processing_stats["memory_count"] == 2
        assert "æŠ€è¡“å­¦ç¿’" in daily_report.overall_summary or "é€²å±•" in daily_report.overall_summary
    
    @pytest.mark.asyncio
    async def test_integrated_message_system(self):
        """çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ãƒœãƒƒãƒˆä½œæˆ
        mock_spectra_bot = MagicMock()
        mock_channel = AsyncMock()
        mock_spectra_bot.get_channel.return_value = mock_channel
        
        output_bots = {"spectra": mock_spectra_bot}
        message_system = IntegratedMessageSystem(output_bots)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ—¥å ±
        from src.core.daily_report_system import DailyReport, DepartmentReport
        sample_report = DailyReport(
            date=datetime.now(),
            departments=[
                DepartmentReport(
                    name="Development",
                    emoji="ğŸ—ƒï¸",
                    themes=["TypeScript"],
                    details=["æŠ€è¡“ç¿’å¾—ä¸­"],
                    progress_score=0.8
                )
            ],
            overall_summary="é–‹ç™ºæ´»å‹•ãŒæ´»ç™º",
            processing_stats={"processing_time": 5.0}
        )
        
        # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ãƒ†ã‚¹ãƒˆ
        success = await message_system.send_integrated_morning_message(
            sample_report, 123456789
        )
        
        # é€ä¿¡æˆåŠŸç¢ºèª
        assert success == True
        mock_channel.send.assert_called_once()
        
        # é€ä¿¡å†…å®¹ç¢ºèª
        call_args = mock_channel.send.call_args
        assert "Morning Meeting" in call_args.kwargs['content']
        assert call_args.kwargs['embed'] is not None


class TestEventDrivenWorkflow:
    """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.asyncio
    async def test_morning_workflow_orchestration(self):
        """çµ±åˆæœæ¬¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±æ‹¬ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        mock_long_term_processor = AsyncMock()
        mock_report_generator = MagicMock()
        mock_message_system = AsyncMock()
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±æ‹¬ã‚·ã‚¹ãƒ†ãƒ 
        orchestrator = EventDrivenWorkflowOrchestrator(
            long_term_memory_processor=mock_long_term_processor,
            daily_report_generator=mock_report_generator,
            integrated_message_system=mock_message_system,
            command_center_channel_id=123456789
        )
        
        # é•·æœŸè¨˜æ†¶å‡¦ç†çµæœãƒ¢ãƒƒã‚¯
        mock_memories = [MagicMock()]
        mock_progress_diff = MagicMock()
        mock_long_term_processor.daily_memory_consolidation.return_value = (mock_memories, mock_progress_diff)
        
        # æ—¥å ±ç”Ÿæˆçµæœãƒ¢ãƒƒã‚¯
        mock_daily_report = MagicMock()
        mock_report_generator.generate_daily_report.return_value = mock_daily_report
        
        # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡æˆåŠŸãƒ¢ãƒƒã‚¯
        mock_message_system.send_integrated_morning_message.return_value = True
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
        success = await orchestrator.execute_morning_workflow()
        
        # å®Ÿè¡Œçµæœç¢ºèª
        assert success == True
        
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œç¢ºèª
        mock_long_term_processor.daily_memory_consolidation.assert_called_once()
        mock_report_generator.generate_daily_report.assert_called_once()
        mock_message_system.send_integrated_morning_message.assert_called_once()


# ãƒ†ã‚¹ãƒˆè¨­å®š
@pytest.fixture(scope="session")
def event_loop():
    """ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    pytest.main([__file__, "-v", "--tb=short"])
#!/usr/bin/env python3
"""
v0.3.0 Long-term Memory System Integration Tests
é•·æœŸè¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆ
"""

import pytest
import asyncio
import os
import uuid
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch
from typing import List, Dict, Any

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.infrastructure.long_term_memory import LongTermMemoryProcessor, ProcessedMemory
from src.infrastructure.deduplication_system import MinHashDeduplicator, MemoryItem
from src.core.daily_report_system import (
    DailyReportGenerator, 
    IntegratedMessageSystem, 
    EventDrivenWorkflowOrchestrator
)


class TestLongTermMemoryProcessor:
    """é•·æœŸè¨˜æ†¶å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def processor(self):
        """LongTermMemoryProcessorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return LongTermMemoryProcessor(
            redis_url="redis://localhost:6379",
            postgres_url="postgresql://test:test@localhost:5432/test_db",
            gemini_api_key="test_api_key"
        )
    
    @pytest.fixture
    def sample_redis_memories(self):
        """ã‚µãƒ³ãƒ—ãƒ«Redisè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿"""
        return [
            {
                "id": str(uuid.uuid4()),
                "content": "TypeScriptã§Reactã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é–‹ç™ºã—ã¦ã„ã¾ã™",
                "timestamp": datetime.now().isoformat(),
                "channel_id": "123456789",
                "user_id": "user001",
                "metadata": {"type": "development"}
            },
            {
                "id": str(uuid.uuid4()),
                "content": "è»¢è·æ´»å‹•ã®é€²æ—ã‚’å ±å‘Šã—ã¾ã™",
                "timestamp": datetime.now().isoformat(),
                "channel_id": "123456789",
                "user_id": "user002",
                "metadata": {"type": "personal"}
            },
            {
                "id": str(uuid.uuid4()),
                "content": "æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’è€ƒãˆã¦ã„ã¾ã™",
                "timestamp": datetime.now().isoformat(),
                "channel_id": "987654321",
                "user_id": "user003",
                "metadata": {"type": "creation"}
            }
        ]
    
    @pytest.mark.asyncio
    async def test_api_quota_check(self, processor):
        """APIä½¿ç”¨é‡åˆ¶é™ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        target_date = datetime.now()
        
        # åˆå›ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ¶é™å†…ï¼‰
        assert processor._check_api_quota(target_date) == True
        
        # APIä½¿ç”¨é‡ã‚’3å›ã«è¨­å®š
        processor.api_usage_count = 3
        processor.last_processing_date = target_date.date()
        
        # åˆ¶é™è¶…éãƒã‚§ãƒƒã‚¯
        assert processor._check_api_quota(target_date) == False
        
        # æ–°ã—ã„æ—¥ã«ãªã£ãŸå ´åˆã®ãƒªã‚»ãƒƒãƒˆ
        next_day = target_date + timedelta(days=1)
        assert processor._check_api_quota(next_day) == True
        assert processor.api_usage_count == 0
    
    @pytest.mark.asyncio
    @patch('src.infrastructure.long_term_memory.redis.from_url')
    async def test_fetch_daily_memories(self, mock_redis, processor, sample_redis_memories):
        """Redisè¨˜æ†¶ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ†ã‚¹ãƒˆ"""
        # Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ¢ãƒƒã‚¯
        mock_redis_client = AsyncMock()
        mock_redis.return_value = mock_redis_client
        
        # Redisã‚­ãƒ¼æ¤œç´¢çµæœãƒ¢ãƒƒã‚¯
        mock_redis_client.keys.return_value = [
            b"memory:2024-01-15:msg001",
            b"memory:2024-01-15:msg002"
        ]
        
        # ãƒ‡ãƒ¼ã‚¿å–å¾—çµæœãƒ¢ãƒƒã‚¯
        mock_redis_client.hgetall.side_effect = [
            {b"id": b"test001", b"content": b"test content 1"},
            {b"id": b"test002", b"content": b"test content 2"}
        ]
        
        target_date = datetime(2024, 1, 15)
        memories = await processor._fetch_daily_memories(target_date)
        
        assert len(memories) == 2
        assert memories[0]["id"] == "test001"
        assert memories[1]["content"] == "test content 2"
        
        # Redisã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ã‚¯ãƒ­ãƒ¼ã‚ºç¢ºèª
        mock_redis_client.close.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('src.infrastructure.long_term_memory.GoogleGenerativeAI')
    async def test_api1_unified_analysis(self, mock_gemini, processor, sample_redis_memories):
        """API 1: Geminiçµ±åˆåˆ†æãƒ†ã‚¹ãƒˆ"""
        # Geminiå¿œç­”ãƒ¢ãƒƒã‚¯
        mock_response = """
        [
          {
            "id": "test001",
            "structured_content": "TypeScript Reacté–‹ç™º",
            "memory_type": "learning",
            "entities": [{"name": "TypeScript", "type": "technology"}],
            "importance_score": 0.8,
            "progress_indicators": {"skill_development": "TypeScript"}
          }
        ]
        """
        
        mock_gemini_instance = AsyncMock()
        mock_gemini_instance.ainvoke.return_value = mock_response
        processor.gemini_flash = mock_gemini_instance
        
        processed = await processor._api1_unified_analysis(sample_redis_memories)
        
        assert len(processed) == 1
        assert processed[0].structured_content == "TypeScript Reacté–‹ç™º"
        assert processed[0].importance_score == 0.8
        assert len(processed[0].entities) == 1
        assert processed[0].entities[0]["name"] == "TypeScript"
        
        # Geminiå‘¼ã³å‡ºã—ç¢ºèª
        mock_gemini_instance.ainvoke.assert_called_once()
    
    @pytest.mark.asyncio
    @patch('src.infrastructure.long_term_memory.GoogleGenerativeAIEmbeddings')
    async def test_api3_batch_embeddings(self, mock_embeddings, processor):
        """API 3: ãƒãƒƒãƒembeddingç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        # ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ¸ˆã¿è¨˜æ†¶
        memories = [
            ProcessedMemory(
                id="test001",
                original_content="test content",
                structured_content="structured test",
                timestamp=datetime.now(),
                channel_id=123,
                user_id="user001",
                memory_type="conversation",
                entities=[],
                importance_score=0.7,
                progress_indicators={},
                metadata={}
            )
        ]
        
        # Embeddingçµæœãƒ¢ãƒƒã‚¯
        mock_embeddings_instance = AsyncMock()
        mock_embeddings_instance.aembed_documents.return_value = [
            [0.1, 0.2, 0.3, 0.4]  # 4æ¬¡å…ƒãƒ†ã‚¹ãƒˆembedding
        ]
        processor.embeddings_client = mock_embeddings_instance
        
        result = await processor._api3_batch_embeddings(memories)
        
        assert len(result) == 1
        assert result[0].embedding == [0.1, 0.2, 0.3, 0.4]
        
        # Embedding APIå‘¼ã³å‡ºã—ç¢ºèª
        mock_embeddings_instance.aembed_documents.assert_called_once_with(["structured test"])


class TestMinHashDeduplicator:
    """é‡è¤‡æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def deduplicator(self):
        """MinHashDeduplicatorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return MinHashDeduplicator(threshold=0.8, num_perm=64)
    
    def test_content_normalization(self):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ­£è¦åŒ–ãƒ†ã‚¹ãƒˆ"""
        from src.infrastructure.deduplication_system import ContentNormalizer
        
        # URLã€ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ã€è¨˜å·ã®é™¤å»ãƒ†ã‚¹ãƒˆ
        original = "Hello @user123! Check https://example.com for more info!!!"
        normalized = ContentNormalizer.normalize_text(original)
        
        assert "https://example.com" not in normalized
        assert "@user123" not in normalized
        assert normalized.lower() == normalized  # å°æ–‡å­—åŒ–ç¢ºèª
    
    def test_memory_deduplication(self, deduplicator):
        """è¨˜æ†¶é‡è¤‡é™¤å»ãƒ†ã‚¹ãƒˆ"""
        # é¡ä¼¼ã—ãŸè¨˜æ†¶ã‚¢ã‚¤ãƒ†ãƒ 
        memory1 = MemoryItem(
            id="mem001",
            content="TypeScriptã§Reactã‚¢ãƒ—ãƒªã‚’é–‹ç™ºã—ã¦ã„ã¾ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation",
            metadata={}
        )
        
        memory2 = MemoryItem(
            id="mem002", 
            content="Reactã‚¢ãƒ—ãƒªã‚’TypeScriptã§é–‹ç™ºä¸­ã§ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation",
            metadata={}
        )
        
        memory3 = MemoryItem(
            id="mem003",
            content="Pythonã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰å®Ÿè£…ã‚’ã—ã¦ã„ã¾ã™",
            timestamp=datetime.now(),
            channel_id=456,
            user_id="user002",
            memory_type="task",
            metadata={}
        )
        
        # ãƒãƒƒãƒé‡è¤‡é™¤å»å®Ÿè¡Œ
        unique_memories = deduplicator.batch_deduplicate([memory1, memory2, memory3])
        
        # é¡ä¼¼è¨˜æ†¶ï¼ˆmemory1, memory2ï¼‰ã¯1ã¤ã«é‡è¤‡é™¤å»ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
        assert len(unique_memories) <= 2
        assert any(mem.id == memory3.id for mem in unique_memories)  # memory3ã¯æ®‹å­˜
    
    def test_similarity_calculation(self, deduplicator):
        """é¡ä¼¼åº¦è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        memory1 = MemoryItem(
            id="mem001",
            content="åŒã˜å†…å®¹ã§ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation", 
            metadata={}
        )
        
        memory2 = MemoryItem(
            id="mem002",
            content="åŒã˜å†…å®¹ã§ã™",
            timestamp=datetime.now(),
            channel_id=123,
            user_id="user001",
            memory_type="conversation",
            metadata={}
        )
        
        # è¨˜æ†¶è¿½åŠ 
        deduplicator.add_memory(memory1)
        deduplicator.add_memory(memory2)
        
        # é¡ä¼¼åº¦è¨ˆç®—
        similarity = deduplicator.get_similarity("mem001", "mem002")
        
        # å®Œå…¨ä¸€è‡´ãªã®ã§é«˜ã„é¡ä¼¼åº¦
        assert similarity > 0.8


class TestDailyReportSystem:
    """æ—¥å ±ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    @pytest.fixture
    def report_generator(self):
        """DailyReportGeneratorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return DailyReportGenerator()
    
    @pytest.fixture
    def sample_processed_memories(self):
        """ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ¸ˆã¿è¨˜æ†¶"""
        return [
            ProcessedMemory(
                id="mem001",
                original_content="TypeScriptå­¦ç¿’ä¸­",
                structured_content="TypeScriptæŠ€è¡“ç¿’å¾—",
                timestamp=datetime.now(),
                channel_id=123,
                user_id="user001",
                memory_type="learning",
                entities=[{"name": "TypeScript", "type": "technology"}],
                importance_score=0.8,
                progress_indicators={"skill_development": "TypeScript"},
                metadata={}
            ),
            ProcessedMemory(
                id="mem002",
                original_content="æ–°ã—ã„ãƒ‡ã‚¶ã‚¤ãƒ³ã‚¢ã‚¤ãƒ‡ã‚¢",
                structured_content="UIãƒ‡ã‚¶ã‚¤ãƒ³ä¼ç”»",
                timestamp=datetime.now(),
                channel_id=456,
                user_id="user002",
                memory_type="progress",
                entities=[{"name": "UIãƒ‡ã‚¶ã‚¤ãƒ³", "type": "creative"}],
                importance_score=0.7,
                progress_indicators={"project_advancement": "ãƒ‡ã‚¶ã‚¤ãƒ³ä¼ç”»"},
                metadata={}
            )
        ]
    
    def test_daily_report_generation(self, report_generator, sample_processed_memories):
        """æ—¥å ±ç”Ÿæˆãƒ†ã‚¹ãƒˆ"""
        from src.infrastructure.long_term_memory import ProgressDifferential
        
        progress_diff = ProgressDifferential(
            date=datetime.now(),
            new_entities=["TypeScript"],
            progressed_entities=["UIãƒ‡ã‚¶ã‚¤ãƒ³"],
            stagnant_entities=[],
            completed_tasks=[],
            new_skills=["TypeScript"],
            overall_summary="æŠ€è¡“å­¦ç¿’ã¨å‰µä½œæ´»å‹•ãŒé€²å±•"
        )
        
        processing_stats = {
            "processing_time": 8.5,
            "memory_count": 2,
            "api_usage": 3
        }
        
        daily_report = report_generator.generate_daily_report(
            sample_processed_memories, progress_diff, processing_stats
        )
        
        assert daily_report.date.date() == datetime.now().date()
        assert len(daily_report.departments) == 3  # Command Center, Creation, Development
        assert daily_report.processing_stats["memory_count"] == 2
        assert "æŠ€è¡“å­¦ç¿’" in daily_report.overall_summary or "é€²å±•" in daily_report.overall_summary
    
    @pytest.mark.asyncio
    async def test_integrated_message_system(self):
        """çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ãƒœãƒƒãƒˆä½œæˆ
        mock_spectra_bot = MagicMock()
        mock_channel = AsyncMock()
        mock_spectra_bot.get_channel.return_value = mock_channel
        
        output_bots = {"spectra": mock_spectra_bot}
        message_system = IntegratedMessageSystem(output_bots)
        
        # ã‚µãƒ³ãƒ—ãƒ«æ—¥å ±
        from src.core.daily_report_system import DailyReport, DepartmentReport
        sample_report = DailyReport(
            date=datetime.now(),
            departments=[
                DepartmentReport(
                    name="Development",
                    emoji="ğŸ—ƒï¸",
                    themes=["TypeScript"],
                    details=["æŠ€è¡“ç¿’å¾—ä¸­"],
                    progress_score=0.8
                )
            ],
            overall_summary="é–‹ç™ºæ´»å‹•ãŒæ´»ç™º",
            processing_stats={"processing_time": 5.0}
        )
        
        # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡ãƒ†ã‚¹ãƒˆ
        success = await message_system.send_integrated_morning_message(
            sample_report, 123456789
        )
        
        # é€ä¿¡æˆåŠŸç¢ºèª
        assert success == True
        mock_channel.send.assert_called_once()
        
        # é€ä¿¡å†…å®¹ç¢ºèª
        call_args = mock_channel.send.call_args
        assert "Morning Meeting" in call_args.kwargs['content']
        assert call_args.kwargs['embed'] is not None


class TestEventDrivenWorkflow:
    """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‰ãƒªãƒ–ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ†ã‚¹ãƒˆ"""
    
    @pytest.mark.asyncio
    async def test_morning_workflow_orchestration(self):
        """çµ±åˆæœæ¬¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±æ‹¬ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¢ãƒƒã‚¯ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        mock_long_term_processor = AsyncMock()
        mock_report_generator = MagicMock()
        mock_message_system = AsyncMock()
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±æ‹¬ã‚·ã‚¹ãƒ†ãƒ 
        orchestrator = EventDrivenWorkflowOrchestrator(
            long_term_memory_processor=mock_long_term_processor,
            daily_report_generator=mock_report_generator,
            integrated_message_system=mock_message_system,
            command_center_channel_id=123456789
        )
        
        # é•·æœŸè¨˜æ†¶å‡¦ç†çµæœãƒ¢ãƒƒã‚¯
        mock_memories = [MagicMock()]
        mock_progress_diff = MagicMock()
        mock_long_term_processor.daily_memory_consolidation.return_value = (mock_memories, mock_progress_diff)
        
        # æ—¥å ±ç”Ÿæˆçµæœãƒ¢ãƒƒã‚¯
        mock_daily_report = MagicMock()
        mock_report_generator.generate_daily_report.return_value = mock_daily_report
        
        # çµ±åˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é€ä¿¡æˆåŠŸãƒ¢ãƒƒã‚¯
        mock_message_system.send_integrated_morning_message.return_value = True
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
        success = await orchestrator.execute_morning_workflow()
        
        # å®Ÿè¡Œçµæœç¢ºèª
        assert success == True
        
        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®å®Ÿè¡Œç¢ºèª
        mock_long_term_processor.daily_memory_consolidation.assert_called_once()
        mock_report_generator.generate_daily_report.assert_called_once()
        mock_message_system.send_integrated_morning_message.assert_called_once()


class TestLongTermMemoryPhase3Integration:
    """Phase 3çµ±åˆæ©Ÿèƒ½ã®å¤±æ•—ãƒ†ã‚¹ãƒˆï¼ˆTDD REDæ®µéšï¼‰"""
    
    @pytest.fixture
    def processor(self):
        """LongTermMemoryProcessorã®ãƒ†ã‚¹ãƒˆãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
        return LongTermMemoryProcessor(
            redis_url="redis://localhost:6379",
            postgres_url="postgresql://test:test@localhost:5432/test_db",
            gemini_api_key="test_api_key"
        )
    
    @pytest.mark.asyncio
    async def test_new_embedding_client_integration_fails(self, processor):
        """æ–°ã—ã„embedding_clientã¨ã®çµ±åˆãŒæœªå®Ÿè£…ã§å¤±æ•—ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        # æœŸå¾…: GoogleEmbeddingClientä½¿ç”¨
        # å®Ÿéš›: ç›´æ¥GoogleGenerativeAIEmbeddingsä½¿ç”¨ï¼ˆçµ±åˆæœªå®Ÿè£…ï¼‰
        
        # embedding_clientã®å‹ç¢ºèª
        assert hasattr(processor, 'embedding_client'), "embedding_clientå±æ€§ãŒå­˜åœ¨ã—ãªã„"
        
        # æ–°ã—ã„GoogleEmbeddingClientã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨ç¢ºèªï¼ˆå¤±æ•—ã™ã‚‹ã¯ãšï¼‰
        from src.infrastructure.embedding_client import GoogleEmbeddingClient
        assert isinstance(processor.embedding_client, GoogleEmbeddingClient), "æ–°ã—ã„GoogleEmbeddingClientãŒæœªçµ±åˆ"
    
    @pytest.mark.asyncio
    async def test_environment_variable_controls_passes(self, processor):
        """DEDUPLICATION_THRESHOLD, API_QUOTA_DAILY_LIMITç’°å¢ƒå¤‰æ•°åˆ¶å¾¡çµ±åˆæˆåŠŸç¢ºèª"""
        
        with patch.dict('os.environ', {
            'DEDUPLICATION_THRESHOLD': '0.9',
            'API_QUOTA_DAILY_LIMIT': '5',
            'MIN_IMPORTANCE_SCORE': '0.6'
        }):
            # æ–°ã—ã„processorã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ
            new_processor = LongTermMemoryProcessor()
            
            # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®èª­ã¿è¾¼ã¿ç¢ºèª
            assert hasattr(new_processor, 'deduplicator'), "deduplicatorå±æ€§ãŒå­˜åœ¨ã—ãªã„"
            assert new_processor.deduplicator.threshold == 0.9, "ç’°å¢ƒå¤‰æ•°DEDUPLICATION_THRESHOLDãŒæœªå¯¾å¿œ"
            
            assert hasattr(new_processor, 'daily_api_limit'), "daily_api_limitå±æ€§ãŒå­˜åœ¨ã—ãªã„"
            assert new_processor.daily_api_limit == 5, "ç’°å¢ƒå¤‰æ•°API_QUOTA_DAILY_LIMITãŒæœªå¯¾å¿œ"
            
            assert hasattr(new_processor, 'min_importance_score'), "min_importance_scoreå±æ€§ãŒå­˜åœ¨ã—ãªã„"
            assert new_processor.min_importance_score == 0.6, "ç’°å¢ƒå¤‰æ•°MIN_IMPORTANCE_SCOREãŒæœªå¯¾å¿œ"
    
    @pytest.mark.asyncio
    async def test_batch_embedding_optimization_passes(self, processor):
        """Phase 1-2ã§æ§‹ç¯‰ã—ãŸæœ€é©åŒ–æ©Ÿèƒ½ï¼ˆãƒªãƒˆãƒ©ã‚¤ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç­‰ï¼‰çµ±åˆæˆåŠŸç¢ºèª"""
        
        # ProcessedMemoryã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ä½œæˆ
        from src.infrastructure.long_term_memory import ProcessedMemory
        sample_memories = [
            ProcessedMemory(
                id="test1", original_content="ãƒ†ã‚¹ãƒˆ1", structured_content="ãƒ†ã‚¹ãƒˆæ–‡æ›¸1",
                timestamp=datetime.now(), channel_id=123, user_id="user1", memory_type="test",
                entities=[], importance_score=0.5, progress_indicators={}, metadata={}
            ),
            ProcessedMemory(
                id="test2", original_content="ãƒ†ã‚¹ãƒˆ2", structured_content="ãƒ†ã‚¹ãƒˆæ–‡æ›¸2",
                timestamp=datetime.now(), channel_id=123, user_id="user2", memory_type="test",
                entities=[], importance_score=0.5, progress_indicators={}, metadata={}
            ),
            ProcessedMemory(
                id="test3", original_content="ãƒ†ã‚¹ãƒˆ3", structured_content="ãƒ†ã‚¹ãƒˆæ–‡æ›¸3",
                timestamp=datetime.now(), channel_id=123, user_id="user3", memory_type="test",
                entities=[], importance_score=0.5, progress_indicators={}, metadata={}
            )
        ]
        
        # _api3_batch_embeddings()ã®å®Ÿè£…ç¢ºèª
        with patch.object(processor, 'api_usage_count', 0):
            # embed_documents_batch()ãƒ¡ã‚½ãƒƒãƒ‰ãŒä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’æœŸå¾…
            with patch.object(processor.embeddings_client, 'embed_documents_batch') as mock_batch:
                mock_batch.return_value = [[0.1] * 768] * 3
                
                result = await processor._api3_batch_embeddings(sample_memories)
                
                # æ–°ã—ã„ãƒãƒƒãƒãƒ¡ã‚½ãƒƒãƒ‰ãŒå‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                mock_batch.assert_called_once()
                assert len(result) == 3, "ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿çµæœãŒæœŸå¾…ã¨ç•°ãªã‚‹"
                assert all(memory.embedding == [0.1] * 768 for memory in result), "embeddingå‰²ã‚Šå½“ã¦ãŒæ­£ã—ããªã„"
    
    @pytest.mark.asyncio
    async def test_daily_memory_redis_key_pattern_passes(self, processor):
        """ãƒ‡ã‚¤ãƒªãƒ¼ãƒ¡ãƒ¢ãƒªç”¨Redisã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒçµ±ä¸€ã•ã‚Œã¦æˆåŠŸ"""
        
        test_date = datetime(2024, 1, 15)
        
        # æœŸå¾…: çµ±ä¸€ã•ã‚ŒãŸã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ "daily_memory:{date}:*"
        
        with patch('src.infrastructure.long_term_memory.redis.from_url') as mock_redis_from_url:
            mock_redis_client = AsyncMock()
            mock_redis_from_url.return_value = mock_redis_client
            
            # çµ±ä¸€ã•ã‚ŒãŸã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿”ã™ãƒ¢ãƒƒã‚¯
            mock_redis_client.keys.return_value = [
                f"daily_memory:{test_date.strftime('%Y-%m-%d')}:summary".encode(),
                f"daily_memory:{test_date.strftime('%Y-%m-%d')}:analysis".encode()
            ]
            mock_redis_client.hgetall.return_value = {
                b'content': b'test content',
                b'timestamp': test_date.isoformat().encode(),
                b'channel_id': b'123',
                b'user_id': b'user1'
            }
            
            # _fetch_daily_memoriesã§ã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª
            memories = await processor._fetch_daily_memories(test_date)
            
            # çµ±ä¸€ã‚­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒä½¿ç”¨ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
            expected_pattern = f"daily_memory:{test_date.strftime('%Y-%m-%d')}:*"
            mock_redis_client.keys.assert_called_once_with(expected_pattern)
    
    @pytest.mark.asyncio
    async def test_enhanced_error_handling_passes(self, processor):
        """Phase 2ã§å®Ÿè£…ã•ã‚ŒãŸå¼·åŒ–ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµ±åˆæˆåŠŸç¢ºèª"""
        
        # Phase 2ã§å®Ÿè£…ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨ç¢ºèª
        from src.infrastructure.memory_system import MemorySystemConnectionError, MemorySystemError
        
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¢ºèª
        test_date = datetime.now()
        
        with patch.object(processor, 'daily_memory_consolidation') as mock_consolidation:
            # Redisæ¥ç¶šã‚¨ãƒ©ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            mock_consolidation.side_effect = MemorySystemConnectionError("Redisæ¥ç¶šå¤±æ•—")
            
            # å¼·åŒ–ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            with pytest.raises(MemorySystemConnectionError) as exc_info:
                await processor.daily_memory_consolidation(test_date)
            
            assert "Redisæ¥ç¶šå¤±æ•—" in str(exc_info.value)
    
    @pytest.mark.asyncio 
    async def test_performance_metrics_integration_passes(self, processor):
        """Phase 2ã§å®Ÿè£…ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬æ©Ÿèƒ½çµ±åˆæˆåŠŸç¢ºèª"""
        
        # performance_metricså±æ€§ã®å­˜åœ¨ç¢ºèª
        assert hasattr(processor, 'performance_metrics'), "performance_metricså±æ€§ãŒå­˜åœ¨ã—ãªã„"
        
        # _measure_performance()ãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ç¢ºèª
        assert hasattr(processor, '_measure_performance'), "_measure_performance()ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ãªã„"
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ã®å®Ÿè¡Œç¢ºèª
        from src.infrastructure.long_term_memory import ProcessedMemory
        sample_memory = ProcessedMemory(
            id="perf_test", original_content="ãƒ†ã‚¹ãƒˆ", structured_content="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ",
            timestamp=datetime.now(), channel_id=123, user_id="user1", memory_type="test",
            entities=[], importance_score=0.5, progress_indicators={}, metadata={}
        )
        
        with patch.object(processor.embeddings_client, 'embed_documents_batch') as mock_batch:
            mock_batch.return_value = [[0.1] * 768]
            
            with patch.object(processor, '_measure_performance', wraps=processor._measure_performance) as mock_measure:
                await processor._api3_batch_embeddings([sample_memory])
                
                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨ˆæ¸¬ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª
                mock_measure.assert_called()


# ãƒ†ã‚¹ãƒˆè¨­å®š
@pytest.fixture(scope="session")
def event_loop():
    """ãƒ†ã‚¹ãƒˆç”¨ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    pytest.main([__file__, "-v", "--tb=short"])